# vim: syntax=python
# Manage copying and performing trivial format conversions of local data
# and downloading external datasets.


# manifest is a pandas data table of all input files + metadata
# # for now, we assume the concatenation of all metadata produces
# # a unique ID for each file.
def make_local_mapping(manifest, namesep="___"):
    manifest_size = len(manifest.index)
    local_name = manifest['celltype'] + namesep + manifest['filetype'] + namesep + manifest['qualtype'] + '.' + manifest['fileext']
    out_to_in = dict(zip(local_name, manifest['filepath']))
    dict_size = len(out_to_in)
    # make sure each line in maniest has one line in the dict -
    # equivalent to making sure the metadata concatenation is unique
    if manifest_size != dict_size:
        raise RuntimeError('metadata in input manifest does not uniquely identify files')
    
    return(out_to_in)


input_copy_mapping = make_local_mapping(manifest)
rule copy_inputs:
    input:
        lambda wildcards: input_copy_mapping[wildcards.celltype + "___" + wildcards.filetype + "___" + wildcards.qualtype + "." + wildcards.fileext]
    resources:
        mem_mb=1000
    output:
        "input/{celltype}___{filetype}___{qualtype}.{fileext}"
    shell:
        """
        cp -n {input} {output}
        """


# Create passA, passAB, etc. RData files from CSV files.
# Results used to be provided in RDA format. Now it's somewhat counterproductive
# to do this but it isn't worth reworking the entire pipeline.
rule mut_csv_to_rda:
    input:
        "scan2_output/{celltype}_snv_indel_pass_rescue.FILTERED.txt"
    output:
        "input/{celltype}___mut___{qualtype}.rda"
    params:
        muttype=lambda wildcards: 'indel' if wildcards.qualtype.startswith('indel') else 'snv',
        rescue=lambda wildcards: '| rescue == TRUE' if wildcards.qualtype.endswith("AB") else ''
    resources:
        mem_mb=1000
    shell:
        """
        Rscript -e 'library(data.table); muts <- fread("{input}"); muts <- muts[muttype == "{params.muttype}" & (pass == TRUE {params.rescue})]; save(muts, file="{output}")'
        """


rule tables_rda_to_csv:
    input:
        "input/{celltype}___{filetype}___{qualtype}.rda",
    output:
        "tables/{celltype}___{filetype}___{qualtype}.csv",
    resources:
        mem_mb=4000
    log:
        "tables/{celltype}___{filetype}___{qualtype}.log"
    script:
        "scripts/rda_to_csv.R"


rule rda_to_vcf:
    input:
        rda="input/{celltype}___mut___{qualtype}.rda"
    output:
        vcf="vcfs/{celltype}___mut___{qualtype}.vcf"
    params:
        samplename='dummy'
    resources:
        mem_mb=4000
    log:
        "vcfs/{celltype}___mut___{qualtype}.rda_to_vcf.log"
    script:
        "scripts/rda_to_vcf.R"


rule download_roadmap_narrowpeak:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{wildcards.eid}-{wildcards.mark}.narrowPeak.gz"
        """


rule download_roadmap_bigwig:
    input:
    output:
        "data/roadmap/histone_marks/bigwig/{eid}-{mark}.fc.signal.bigwig"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/signal/consolidated/macs2signal/foldChange/{wildcards.eid}-{wildcards.mark}.fc.signal.bigwig"
        """


rule download_encode_replichip_bigwig:
    input:
    output:
        "data/encode/replichip/bigwig/{enc_id}.bigwig"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://www.encodeproject.org/files/{wildcards.enc_id}/@@download/{wildcards.enc_id}.bigWig"
        """


rule make_gtex_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="input/any___gtex_expression_tpm___none.gct"
    output:
        bigwig="data/gtex/bigwig/{tissue}.bigwig"
    log:
        "data/gtex/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem_mb=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


rule map_scrnaseq_to_gtex_gene_model:
    input:
        scrnaseq='input/any___scrnaseq_mean_expression___none.csv',
        gct='input/any___gtex_expression_tpm___none.gct'
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_matrix.log"
    resources:
        mem_mb=16000
    script:
        "scripts/map_scrnaseq_to_gtex_gene_model.R"


rule combine_scrnaseq_by_celltype:
    input:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.log"
    resources:
        mem_mb=8000
    script:
        "scripts/combine_scrna_by_celltype.R"


def get_scrnaseq_matrix_file(wildcards):
    # This hack is to enable the combined expression profiles per cell
    # type across the several scRNAseq libraries.
    # Could be done in a better way.
    if wildcards.tissue in [ 'Astrocytes', 'Microglia', 'Endothelial', 'Oligodendrocytes', 'OPCs', 'Neurons', 'Inhibitory-Neurons', 'Excitatory-Neurons']:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    else:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"

    return({ "gtf": "input/any___gtex_gene_model___none.gtf", "gct": gct })


rule make_scrnaseq_signal_bigwig:
    input:
        #gtf="input/any___gtex_gene_model___none.gtf",
        #gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
        unpack(get_scrnaseq_matrix_file)
    output:
        bigwig="data/scrnaseq/expression/bigwig/{tissue}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem_mb=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"



# Exactly the same as above rule, just using a different input gct
# Probably a better way to do this.
rule make_scrnaseq_by_celltype_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    output:
        bigwig="data/scrnaseq/expression/bigwig/{celltype}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{celltype}.log"
    params:
        tissue="{celltype}"
    resources:
        mem_mb=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


methyltype_to_path = {
    'WGBS_FractionalMethylation' : 'WGBS/FractionalMethylation',
    'WGBS_ReadCoverage' : 'WGBS/ReadCoverage',
    'RRBS_FractionalMethylation' : 'RRBS/FractionalMethylation',
    'RRBS_ReadCoverage' : 'RRBS/ReadCoverage'
}

rule download_roadmap_dnamethyl:
    input:
    output:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.bigwig"
    params:
        path=lambda wildcards: methyltype_to_path[wildcards.methyltype]
    log:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.log"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byDataType/dnamethylation/{params.path}_bigwig/$outfile"
        """


# Both downloads the data and splits it back into its original 200bp bins.
# By using a standard bin size, all 100+ epigenomes can be analyzed by a
# single bedenrich run.
# The alignability tiles here are ONLY intended to split the data into its
# original 200bp bins - the BEDs are still filtered afterward for 100bp
# alignability.
rule download_roadmap_chromhmm15:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/15state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed"),
    resources:
        mem_mb=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/coreMarks/jointModel/final/{wildcards.eid}_15_coreMarks_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_chromhmm18:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/18state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed"),
    resources:
        mem_mb=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/core_K27ac/jointModel/final/{wildcards.eid}_18_core_K27ac_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_histone_peaks:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{eid}-{mark}.narrowPeak.gz
        """


rule download_icgc_muts:
    input:
    output:
        "data/icgc/final_consensus_passonly.snv_mnv_indel.icgc.public.maf"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output}.gz \
            "https://dcc.icgc.org/api/v1/download?fn=/PCAWG/consensus_snv_indel/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
        gunzip -c {output}.gz > {output}
        """


rule split_cancer_muts_by_type:
    input:
        tcga="input/any___cancer_tcga___none.maf",
        icgc="input/any___cancer_icgc___none.maf"
    output:
        "data/cancer_snvdens/snvs/{tumor}.txt"
    params:
        tumor="{tumor}"
    resources:
        mem_mb=1000
    shell:
        """
        (echo "chr,pos,refnt,altnt,sample"|tr ',' '\t' ;
         cat {input.tcga} {input.icgc} | cut -f 2,3,4,7,8,10,42,43 | grep "{params.tumor}" | grep SNP | cut -f1,2,5,6,8) > {output}
        """


rule make_cancer_snvdens_bigwigs:
    input:
        txt="data/cancer_snvdens/snvs/{tissue}.txt"
    output:
        sumbigwig="data/cancer_snvdens/bigwig/{tissue}_sumdens.bigwig",
        normbigwig="data/cancer_snvdens/bigwig/{tissue}_normdens.bigwig"
    log:
        "data/cancer_snvdens/bigwig/{tissue}.log"
    benchmark:
        "data/cancer_snvdens/bigwig/{tissue}.benchmark.txt"
    resources:
        # this extreme memory req is because a 100bp bin x #samples matrix is built
        # cancer types with 200+ samples fail at 96G and I don't know how much memory is truly needed
        # so this very large number is to avoid yet another failure.
        #mem_mb=lambda wildcards, attempt: 32000 if attempt==1 else 196000
        mem_mb=196000
    script:
        "scripts/make_cancer_mutdens_bigwigs.R"


rule download_gencode_genes:
    input:
    output:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output}.gz "https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v26lift37.annotation.gtf.gz"
        gunzip -c {output}.gz > {output}
        """



rule make_gtex_gene_model:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        script="data/gtex/collapse_annotation.py",
        gtf="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.gtf",
        gtf_genes="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output.script} \
            "https://raw.githubusercontent.com/broadinstitute/gtex-pipeline/master/gene_model/collapse_annotation.py"
        python {output.script} {input} {output.gtf}
        awk '$0 ~ /^#/ || $3 == "gene"' {output.gtf} > {output.gtf_genes}
        """



rule download_nott_bigwigs:
    input:
    output:
        "data/nott/bigwig/{file}"
    params:
        file="{file}",
        mark=lambda wildcards: 'atac' if 'atac' in wildcards.file else ('h3k27ac' if 'H3K27ac' in wildcards.file else 'h3k4me3')
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "http://homer.ucsd.edu/hubs/nuclei_{params.mark}_hg19_pooled/hg19/{params.file}"
        """


ruleorder: copyover_nott_supptab5 > download_nott_supptab5
rule copyover_nott_supptab5:
    input:
        "external_data/aay0793-nott-table-s5.xlsx"
    output:
        "data/nott/SupplementaryTable5.xlsx"
    resources:
        mem_mb=100
    shell:
        "cp -n {input} {output}"


rule download_nott_supptab5:
    input:
    output:
        "data/nott/SupplementaryTable5.xlsx"
    resources:
        mem_mb=1000
    message:
        "This rule will always fail because science.org blocks simple wget downloads. Please open `https://www.science.org/doi/suppl/10.1126/science.aay0793/suppl_file/aay0793-nott-table-s5.xlsx` in a web browser and place the file in data/nott/SupplementaryTable5.xlsx"
    shell:
        """
        wget -O {output} \
            "https://www.science.org/doi/suppl/10.1126/science.aay0793/suppl_file/aay0793-nott-table-s5.xlsx"
        """


rule parse_nott_supptab5:
    input:
        xlsx="data/nott/SupplementaryTable5.xlsx"
    output:
        temp(expand('data/nott/bed/{celltype}_{regiontype}.csv',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ])),
        expand('data/nott/bed/{celltype}_{regiontype}.bed',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ])
    log:
        'data/nott/bed/parse_nott_supptab5.log'
    params:
        out_prefix='data/nott/bed'
    resources:
        mem_mb=1000
    script:
        'scripts/parse_nott2019_supptable.R'


rule make_final_nott_bed:
    input:
        beds=expand('data/nott/bed/{celltype}_{regiontype}.bed',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ]), 
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        'data/nott/bed/enhancers_and_promoters_all_celltypes.bed'
    resources:
        mem_mb=1000
    shell:
        """
        cat {input.beds} | bedtools sort -g {input.genome_ordering} > {output}
        """


rule download_gtex_expression:
    input:
    output:
        "data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output}.gz \
            "https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz"
        gunzip -c {output}.gz > {output}
        """


rule download_ucsc_conservation:
    input:
    output:
        phastcons="data/ucsc/conservation/hg19.100way.phastCons.bw",
        phylop="data/ucsc/conservation/hg19.100way.phyloP100way.bw"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output.phylop} "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/phyloP100way/hg19.100way.phyloP100way.bw"
        wget -O {output.phastcons} "https://hgdownload.cse.ucsc.edu/goldenPath/hg19/phastCons100way/hg19.100way.phastCons.bw"
        """



rule download_repliseq:
    input:
    output:
        "data/repliseq/bigwig/{file}"
    params:
        file="{file}"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeUwRepliSeq/{params.file}"
        """


import pandas as pd

scatacseq_meta = pd.read_csv('manifests/SCATACSEQ.MANIFEST')

# Not trivial to generalize because the whole set of background bigwigs
# needs to be known.
rule make_foldchange_qbed_scatacseq:
    input:
        target="enrichment/scatacseq/quantile/qbed/scatacseq___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed",
        background=expand("enrichment/scatacseq/quantile/qbed/scatacseq___librarymerged___merged___{scatacseq_celltype}.{{binsize}}binsize_{{nquantiles}}quantiles.qbed",
            scatacseq_celltype=sorted(set(scatacseq_meta['celltype'])))
    output:
        qbed="enrichment/scatacseq_foldchange/quantile/qbed/scatacseq_foldchange___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed"
    log:
        "enrichment/scatacseq_foldchange/quantile/qbed/scatacseq_foldchange___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.log"
    resources:
        mem_mb=8000
    script:
        "scripts/make_foldchange_qbed.R"



scrnaseq_meta = pd.read_csv('manifests/SCRNASEQ_EXPRESSION_FOLDCHANGE.MANIFEST')

# Not trivial to generalize because the whole set of background bigwigs
# needs to be known.


rule make_foldchange_qbed_scrnaseq:
    input:
        target="enrichment/scrnaseq_expression_mc08/quantile/qbed/scrnaseq___expression___combined___combined___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed",
        background=expand("enrichment/scrnaseq_expression_mc08/quantile/qbed/scrnaseq___expression___combined___combined___{scrnaseq_celltype}.{{binsize}}binsize_{{nquantiles}}quantiles.qbed",
            scrnaseq_celltype=sorted(set(scrnaseq_meta['celltype'])))
    output:
        qbed="enrichment/scrnaseq_expression_mc08_foldchange/quantile/qbed/scrnaseq_foldchange___expression___combined___combined___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed"
    log:
        "enrichment/scrnaseq_expression_mc08_foldchange/quantile/qbed/scrnaseq_foldchange___expression___combined___combined___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.log"
    resources:
        mem_mb=8000
    script:
        "scripts/make_foldchange_qbed.R"



# Both copy the MAF file and remove carriage returns because maf2vcf.pl
# can't handle them.
ruleorder: pcawg_copy_icgc_maf > copy_inputs
rule pcawg_copy_icgc_maf:
    input:
        input_copy_mapping["any___cancer_icgc___none.maf"]
    output:
        "input/any___cancer_icgc___none.maf"
    resources:
        mem_mb=1000
    shell:
        """
        cat {input} | tr -d '\r' > {output}
        """

ruleorder: pcawg_copy_tcga_maf > copy_inputs
rule pcawg_copy_tcga_maf:
    input:
        input_copy_mapping["any___cancer_tcga___none.maf"]
    output:
        "input/any___cancer_tcga___none.maf"
    resources:
        mem_mb=1000
    shell:
        """
        cat {input} | tr -d '\r' > {output}
        """


rule pcawg_get_donor_ids:
    input:
        "input/any___{project}___none.maf",
    output:
        "data/pcawg/any___{project}___donor_ids.txt"
    resources:
        mem_mb=1000
    shell:
        """
        ( head -1 {input} | cut -f42,43 ;
          tail -n +2 {input} | cut -f42,43 | uniq | sort | uniq)  > {output}
        """


rule pcawg_make_metadata:
    input:
        ss="metadata/pcawg_sample_sheet.tsv",
        icgc="data/pcawg/any___cancer_icgc___donor_ids.txt",
        tcga="data/pcawg/any___cancer_tcga___donor_ids.txt"
    output:
        "metadata/pcawg_metadata.csv"
    resources:
        mem_mb=4000
    shell:
        "snakemake/scripts/make_pcawg_metadata.R {input.ss} {input.icgc} {input.tcga} {output}"


# Allow script to run (and presumably create this file) by specifying
# --config make_pcawg_metadata=1 --until metadata/pcawg_metadata.csv
if 'make_pcawg_metadata' not in config.keys():
    pcawg_meta = pd.read_csv('metadata/pcawg_metadata.csv')
    # make a donor_id -> {tcga|icgc} mapping
    proj_map = dict(zip(list(pcawg_meta['icgc_donor_id']), list(pcawg_meta['project'])))
    pcawg_tumors = sorted(set(pcawg_meta['tumor']))

rule pcawg_extract_sample_vcf:
    input:
        lambda wildcards: "input/any___cancer_" + proj_map[wildcards.donor] + '___none.maf'
    output:
        maf=temp("data/pcawg/sample_vcfs/{donor}.maf"),
        pairs=temp("data/pcawg/sample_vcfs/{donor}.pairs.tsv"),
        vcf="data/pcawg/sample_vcfs/{donor}.vcf"
    log:
        "data/pcawg/sample_vcfs/{donor}.log"
    benchmark:
        "data/pcawg/sample_vcfs/{donor}.benchmark.txt"
    params:
        donor_id='{donor}'
    resources:
        mem_mb=4000
    shell:
        """
        snakemake/scripts/pcawg_extract_one_maf.sh \
            {params.donor_id} \
            {input} \
            {output.maf} \
            data/pcawg/sample_vcfs
        """


rule pcawg_make_tumor_vcf:
    input:
        vcfs=lambda wildcards: list(pcawg_meta.loc[(pcawg_meta.tumor == wildcards.tumor) & (pcawg_meta.maf_processing_error == False), 'file'])
    output:
        "data/pcawg/tumor_vcfs/{tumor}.vcf"
    resources:
        # needs the entire VCF in RAM for sort
        mem_mb=8000
    params:
        tumor="{tumor}"
    shell:
        """
        snakemake/scripts/make_pcawg_tumor_vcf.sh {params.tumor} {input} > {output}
        """


rule make_gencode_gene_regions:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        "data/gencode/gene_regions.bed"
    resources:
        mem_mb=8000
    shell:
        "snakemake/scripts/make_gene_regions.R {input} {output}"


rule simplify_gencode_gene_regions:
    input:
        "data/gencode/gene_regions.bed"
    output:
        "data/gencode/gene_regions_simplified.bed"
    resources:
        mem_mb=1000
    shell:
        """
        sed -e 's/upstream$/intergenic/g' \
            -e 's/downstream$/intergenic/g' \
            -e 's/cds$/exonic/g' \
            -e 's/utr5$/exonic/g' \
            -e 's/utr3$/exonic/g' \
            -e 's/intron$/intronic/g' {input} > {output}
        """




rule make_gencode_gene_bed:
    input:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    output:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.bed"
    log:
        "data/gtex/make_gencode_gene_bed.log"
    resources:
        mem_mb=1000
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ if ($1 ~ /chr[0-9]+/) {{ match($0, /gene_name [^;]*/);  print $1, $4, $5, substr($0, RSTART, RLENGTH); }} }}' {input} \
            | sed -e 's/gene_name //' \
            | tr -d '"' > {output}
        """


rule make_gencode_gene_bed_protein_coding:
    input:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    output:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.protein_coding.bed"
    log:
        "data/gtex/make_gencode_gene_bed.protein_coding.log"
    resources:
        mem_mb=1000
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ if ($1 ~ /chr[0-9]+/ && $0 ~ /gene_type "protein_coding";/) {{ match($0, /gene_name [^;]*/);  print $1, $4, $5, substr($0, RSTART, RLENGTH); }} }}' {input} \
            | sed -e 's/gene_name //' \
            | tr -d '"' > {output}
        """


ruleorder: subset_by_mutsig > copy_inputs
rule subset_by_mutsig:
    input:
        rda="input/{celltype}___{filetype}___{qualtype}.{fileext}"
    output:
        rda="input/{celltype}_{sig}___{filetype}___{qualtype}.{fileext}"
    log:
        "input/{celltype}_{sig}___{filetype}___{qualtype}.{fileext}.log"
    params:
        sig='{sig}'
    resources:
        mem_mb=12000
    script:
        "scripts/subset_by_mutsig.R"



rule machado2022_sbsblood_wget:
    input:
    output:
        "data/machado2022_sbsblood/Machado2022_SupplementaryTables_All.xlsx"
    log:
        "data/machado2022_sbsblood/wget.log"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9402440/bin/41586_2022_5072_MOESM4_ESM.xlsx'
        """


rule machado2022_sbsblood_convert:
    input:
        xlsx="data/machado2022_sbsblood/Machado2022_SupplementaryTables_All.xlsx"
    output:
        supptab8="data/machado2022_sbsblood/SupplementaryTable8.csv",
        csv="data/machado2022_sbsblood/SBSblood.csv",
        pdf="data/machado2022_sbsblood/SBSblood.pdf",
        svg="data/machado2022_sbsblood/SBSblood.svg"
    log:
        "data/machado2022_sbsblood/convert.log"
    params:
        sheet_number=8
    resources:
        mem_mb=1000
    script:
        "scripts/get_machado2022_sbsblood.R"


rule leesix2018_hspc_spectrum_get:
    input:
    output:
        "data/leesix2018_hspc/All_mutations_SBS_indel.txt"
    log:
        "data/leesix2018_hspc/wget.log"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            'https://data.mendeley.com/public-files/datasets/yzjw2stk7f/files/46faf00b-02de-44e7-a5ea-bf84e14e875e/file_downloaded'
        """



rule leesix2018_hspc_spectrum_convert:
    input:
        txt="data/leesix2018_hspc/All_mutations_SBS_indel.txt"
    output:
        csv="data/leesix2018_hspc/HSPC_spectrum.csv",
        pdf="data/leesix2018_hspc/HSPC_spectrum.pdf",
        svg="data/leesix2018_hspc/HSPC_spectrum.svg",
    log:
        "data/leesix2018_hspc/convert.log"
    resources:
        mem_mb=1000
    script:
        "scripts/get_leesix2018_hspc.R"



rule copyover_boca2_peaks:
    input:
        GABA="external_data/GABA_DLPFC.bed.gz",
        GLU="external_data/GLU_DLPFC.bed.gz",
        OLIG="external_data/OLIG_DLPFC.bed.gz",
        MGAS="external_data/MGAS_DLPFC.bed.gz",
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        "data/boca2/OCRs_DLPFC.bed"
    resources:
        mem_mb=1000
    shell:
        """
        (gunzip -c {input.GABA} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "GABA"; }}';
         gunzip -c {input.GLU} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "GLU"; }}';
         gunzip -c {input.OLIG} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "OLIG"; }}';
         gunzip -c {input.MGAS} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "MGAS"; }}') \
        | bedtools sort -g {input.genome_ordering} > {output}
        """


rule download_sarseq_hotspots:
    input:
    output:
        gzbed=temp("data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed.gz"),
        bed="data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output.gzbed} \
            "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE167257&format=file&file=GSE167257%5FSARseq%5FiNeuron%5FOverlapRep123%2Epeaks%2Ebed%2Egz"
        gunzip -c {output.gzbed} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "SAR-seq"; }}' > {output.bed}
        """


rule copyover_repairseq_supptab:
    input:
        "external_data/abb9032_Table_S1.xlsx"
    output:
        "data/dna_repair_hotspots/Reid2021_SupplementaryTable1.xlsx"
    resources:
        mem_mb=1000
    shell:
        "cp -n {input} {output}"


rule parse_repairseq_supptab:
    input:
        xlsx="data/dna_repair_hotspots/Reid2021_SupplementaryTable1.xlsx"
    output:
        csv=temp("data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.csv"),
        bed="data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.bed",
    log:
        'data/nott/dna_repair_hotspots/parse_reid2021_supptable.log'
    resources:
        mem_mb=1000
    params:
        out_prefix='data/dna_repair_hotspots'
    script:
        'scripts/parse_reid2021_supptable.R'


rule make_final_dna_repair_hotspot_bed:
    input:
        repairseq="data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.bed",
        sarseq="data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed",
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        "data/dna_repair_hotspots/DNArepair_hotspots.bed"
    resources:
        mem_mb=1000
    shell:
        """
        cat {input.repairseq} {input.sarseq} \
            | bedtools sort -g {input.genome_ordering} > {output}
        """
