# vim: syntax=python
# Manage copying and performing trivial format conversions of local data
# and downloading external datasets.


# manifest is a pandas data table of all input files + metadata
# # for now, we assume the concatenation of all metadata produces
# # a unique ID for each file.
def make_local_mapping(manifest, namesep="___"):
    manifest_size = len(manifest.index)
    local_name = manifest['celltype'] + namesep + manifest['filetype'] + namesep + manifest['qualtype'] + '.' + manifest['fileext']
    out_to_in = dict(zip(local_name, manifest['filepath']))
    dict_size = len(out_to_in)
    # make sure each line in maniest has one line in the dict -
    # equivalent to making sure the metadata concatenation is unique
    if manifest_size != dict_size:
        raise RuntimeError('metadata in input manifest does not uniquely identify files')
    
    return(out_to_in)


input_copy_mapping = make_local_mapping(manifest)
rule copy_inputs:
    input:
        lambda wildcards: input_copy_mapping[wildcards.celltype + "___" + wildcards.filetype + "___" + wildcards.qualtype + "." + wildcards.fileext]
    resources:
        mem=1000
    output:
        "input/{celltype}___{filetype}___{qualtype}.{fileext}"
    shell:
        """
        cp -n {input} {output}
        """


# Create passA, passAB, etc. RData files from CSV files.
# Results used to be provided in RDA format. Now it's somewhat counterproductive
# to do this but it isn't worth reworking the entire pipeline.
rule mut_csv_to_rda:
    input:
        "scan2_output/{celltype}_snv_indel_pass_rescue.txt"
    output:
        "input/{celltype}___mut___{qualtype}.rda"
    params:
        muttype=lambda wildcards: 'indel' if wildcards.qualtype.startswith('indel') else 'snv',
        rescue=lambda wildcards: '| rescue == TRUE' if wildcards.qualtype.endswith("AB") else ''
    resources:
        mem=1000
    shell:
        """
        Rscript -e 'library(data.table); muts <- fread("{input}"); muts <- muts[muttype == "{params.muttype}" & (pass == TRUE {params.rescue})]; save(muts, file="{output}")'
        """


rule tables_rda_to_csv:
    input:
        "input/{celltype}___{filetype}___{qualtype}.rda",
    output:
        "tables/{celltype}___{filetype}___{qualtype}.csv",
    resources:
        mem=4000
    log:
        "tables/{celltype}___{filetype}___{qualtype}.log"
    script:
        "scripts/rda_to_csv.R"


rule rda_to_vcf:
    input:
        "input/{celltype}___mut___{qualtype}.rda"
    output:
        "vcfs/{celltype}___mut___{qualtype}.vcf"
    resources:
        mem=4000
    log:
        "vcfs/{celltype}___mut___{qualtype}.rda_to_vcf.log"
    script:
        "scripts/rda_to_vcf.R"


rule download_roadmap_narrowpeak:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{wildcards.eid}-{wildcards.mark}.narrowPeak.gz"
        """


rule download_roadmap_bigwig:
    input:
    output:
        "data/roadmap/histone_marks/bigwig/{eid}-{mark}.fc.signal.bigwig"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/signal/consolidated/macs2signal/foldChange/{wildcards.eid}-{wildcards.mark}.fc.signal.bigwig"
        """


rule download_encode_replichip_bigwig:
    input:
    output:
        "data/encode/replichip/bigwig/{enc_id}.bigwig"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://www.encodeproject.org/files/{wildcards.enc_id}/@@download/{wildcards.enc_id}.bigWig"
        """


rule make_gtex_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="input/any___gtex_expression_tpm___none.gct"
    output:
        bigwig="data/gtex/bigwig/{tissue}.bigwig"
    log:
        "data/gtex/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


rule map_scrnaseq_to_gtex_gene_model:
    input:
        scrnaseq='input/any___scrnaseq_mean_expression___none.csv',
        gct='input/any___gtex_expression_tpm___none.gct'
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_matrix.log"
    resources:
        mem=16000
    script:
        "scripts/map_scrnaseq_to_gtex_gene_model.R"


rule combine_scrnaseq_by_celltype:
    input:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.log"
    resources:
        mem=8000
    script:
        "scripts/combine_scrna_by_celltype.R"


def get_scrnaseq_matrix_file(wildcards):
    # This hack is to enable the combined expression profiles per cell
    # type across the several scRNAseq libraries.
    # Could be done in a better way.
    if wildcards.tissue in [ 'Astrocytes', 'Microglia', 'Endothelial', 'Oligodendrocytes', 'OPCs', 'Neurons', 'Inhibitory-Neurons', 'Excitatory-Neurons']:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    else:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"

    return({ "gtf": "input/any___gtex_gene_model___none.gtf", "gct": gct })


rule make_scrnaseq_signal_bigwig:
    input:
        #gtf="input/any___gtex_gene_model___none.gtf",
        #gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
        unpack(get_scrnaseq_matrix_file)
    output:
        bigwig="data/scrnaseq/expression/bigwig/{tissue}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"



# Exactly the same as above rule, just using a different input gct
# Probably a better way to do this.
rule make_scrnaseq_by_celltype_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    output:
        bigwig="data/scrnaseq/expression/bigwig/{celltype}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{celltype}.log"
    params:
        tissue="{celltype}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


methyltype_to_path = {
    'WGBS_FractionalMethylation' : 'WGBS/FractionalMethylation',
    'WGBS_ReadCoverage' : 'WGBS/ReadCoverage',
    'RRBS_FractionalMethylation' : 'RRBS/FractionalMethylation',
    'RRBS_ReadCoverage' : 'RRBS/ReadCoverage'
}

rule download_roadmap_dnamethyl:
    input:
    output:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.bigwig"
    params:
        path=lambda wildcards: methyltype_to_path[wildcards.methyltype]
    log:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.log"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byDataType/dnamethylation/{params.path}_bigwig/$outfile"
        """


# Both downloads the data and splits it back into its original 200bp bins.
# By using a standard bin size, all 100+ epigenomes can be analyzed by a
# single bedenrich run.
# The alignability tiles here are ONLY intended to split the data into its
# original 200bp bins - the BEDs are still filtered afterward for 100bp
# alignability.
rule download_roadmap_chromhmm15:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/15state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed"),
    resources:
        mem=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/coreMarks/jointModel/final/{wildcards.eid}_15_coreMarks_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_chromhmm18:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/18state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed"),
    resources:
        mem=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/core_K27ac/jointModel/final/{wildcards.eid}_18_core_K27ac_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_histone_peaks:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{eid}-{mark}.narrowPeak.gz
        """


rule download_icgc_muts:
    input:
    output:
        "data/icgc/final_consensus_passonly.snv_mnv_indel.icgc.public.maf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz \
            "https://dcc.icgc.org/api/v1/download?fn=/PCAWG/consensus_snv_indel/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
        gunzip -c {output}.gz > {output}
        """


rule split_cancer_muts_by_type:
    input:
        tcga="input/any___cancer_tcga___none.maf",
        icgc="input/any___cancer_icgc___none.maf"
    output:
        "data/cancer_snvdens/snvs/{tumor}.txt"
    params:
        tumor="{tumor}"
    resources:
        mem=1000
    shell:
        """
        (echo "chr,pos,refnt,altnt,sample"|tr ',' '\t' ;
         cat {input.tcga} {input.icgc} | cut -f 2,3,4,7,8,10,42,43 | grep "{params.tumor}" | grep SNP | cut -f1,2,5,6,8) > {output}
        """


rule make_cancer_snvdens_bigwigs:
    input:
        txt="data/cancer_snvdens/snvs/{tissue}.txt"
    output:
        sumbigwig="data/cancer_snvdens/bigwig/{tissue}_sumdens.bigwig",
        normbigwig="data/cancer_snvdens/bigwig/{tissue}_normdens.bigwig"
    log:
        "data/cancer_snvdens/bigwig/{tissue}.log"
    benchmark:
        "data/cancer_snvdens/bigwig/{tissue}.benchmark.txt"
    resources:
        # this extreme memory req is because a 100bp bin x #samples matrix is built
        # cancer types with 200+ samples fail at 96G and I don't know how much memory is truly needed
        # so this very large number is to avoid yet another failure.
        #mem=lambda wildcards, attempt: 32000 if attempt==1 else 196000
        mem=196000
    script:
        "scripts/make_cancer_mutdens_bigwigs.R"


rule download_gencode_genes:
    input:
    output:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz "https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v26lift37.annotation.gtf.gz"
        gunzip -c {output}.gz > {output}
        """



rule make_gtex_gene_model:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        script="data/gtex/collapse_annotation.py",
        gtf="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.gtf",
        gtf_genes="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output.script} \
            "https://raw.githubusercontent.com/broadinstitute/gtex-pipeline/master/gene_model/collapse_annotation.py"
        python {output.script} {input} {output.gtf}
        awk '$0 ~ /^#/ || $3 == "gene"' {output.gtf} > {output.gtf_genes}
        """



rule download_nott_bigwigs:
    input:
    output:
        "data/nott/bigwig/{file}"
    params:
        file="{file}",
        mark=lambda wildcards: 'atac' if 'atac' in wildcards.file else ('h3k27ac' if 'H3K27ac' in wildcards.file else 'h3k4me3')
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "http://homer.ucsd.edu/hubs/nuclei_{params.mark}_hg19_pooled/hg19/{params.file}"
        """



rule download_gtex_expression:
    input:
    output:
        "data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz \
            "https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz"
        gunzip -c {output}.gz > {output}
        """


rule download_ucsc_conservation:
    input:
    output:
        phastcons="data/ucsc/conservation/hg19.100way.phastCons.bw",
        phylop="data/ucsc/conservation/hg19.100way.phyloP100way.bw"
    resources:
        mem=1000
    shell:
        """
        wget -O {output.phylop} "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/phyloP100way/hg19.100way.phyloP100way.bw"
        wget -O {output.phastcons} "https://hgdownload.cse.ucsc.edu/goldenPath/hg19/phastCons100way/hg19.100way.phastCons.bw"
        """



rule download_repliseq:
    input:
    output:
        "data/repliseq/bigwig/{file}"
    params:
        file="{file}"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeUwRepliSeq/{params.file}"
        """


import pandas as pd

scatacseq_meta = pd.read_csv('manifests/SCATACSEQ.MANIFEST')

# Not trivial to generalize because the whole set of background bigwigs
# needs to be known.
rule make_foldchange_qbed_scatacseq:
    input:
        target="enrichment/scatacseq/quantile/qbed/scatacseq___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed",
        background=expand("enrichment/scatacseq/quantile/qbed/scatacseq___librarymerged___merged___{scatacseq_celltype}.{{binsize}}binsize_{{nquantiles}}quantiles.qbed",
            scatacseq_celltype=sorted(set(scatacseq_meta['celltype'])))
    output:
        qbed="enrichment/scatacseq_foldchange/quantile/qbed/scatacseq_foldchange___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed"
    log:
        "enrichment/scatacseq_foldchange/quantile/qbed/scatacseq_foldchange___librarymerged___merged___{scatacseq_celltype}.{binsize}binsize_{nquantiles}quantiles.log"
    resources:
        mem=8000
    script:
        "scripts/make_foldchange_qbed.R"



scrnaseq_meta = pd.read_csv('manifests/SCRNASEQ_EXPRESSION_FOLDCHANGE.MANIFEST')

# Not trivial to generalize because the whole set of background bigwigs
# needs to be known.
rule make_foldchange_qbed_scrnaseq:
    input:
        target="enrichment/scrnaseq_expression_{mincov}/quantile/qbed/scrnaseq___expression___UMB1465___DAPI___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed",
        background=expand("enrichment/scrnaseq_expression_{{mincov}}/quantile/qbed/scrnaseq___expression___UMB1465___DAPI___{scrnaseq_celltype}.{{binsize}}binsize_{{nquantiles}}quantiles.qbed",
            scrnaseq_celltype=sorted(set(scrnaseq_meta['celltype'])))
    output:
        qbed="enrichment/scrnaseq_expression_{mincov}_foldchange/quantile/qbed/scrnaseq_foldchange___expression___UMB1465___DAPI___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.qbed"
    log:
        "enrichment/scrnaseq_expression_{mincov}_foldchange/quantile/qbed/scrnaseq_foldchange___expression___UMB1465___DAPI___{scrnaseq_celltype}.{binsize}binsize_{nquantiles}quantiles.log"
    resources:
        mem=8000
    script:
        "scripts/make_foldchange_qbed.R"



# Both copy the MAF file and remove carriage returns because maf2vcf.pl
# can't handle them.
ruleorder: pcawg_copy_icgc_maf > copy_inputs
rule pcawg_copy_icgc_maf:
    input:
        input_copy_mapping["any___cancer_icgc___none.maf"]
    output:
        "input/any___cancer_icgc___none.maf"
    resources:
        mem=1000
    shell:
        """
        cat {input} | tr -d '\r' > {output}
        """

ruleorder: pcawg_copy_tcga_maf > copy_inputs
rule pcawg_copy_tcga_maf:
    input:
        input_copy_mapping["any___cancer_tcga___none.maf"]
    output:
        "input/any___cancer_tcga___none.maf"
    resources:
        mem=1000
    shell:
        """
        cat {input} | tr -d '\r' > {output}
        """


rule pcawg_get_donor_ids:
    input:
        "input/any___{project}___none.maf",
    output:
        "data/pcawg/any___{project}___donor_ids.txt"
    resources:
        mem=1000
    shell:
        """
        ( head -1 {input} | cut -f42,43 ;
          tail -n +2 {input} | cut -f42,43 | uniq | sort | uniq)  > {output}
        """


rule pcawg_make_metadata:
    input:
        ss="metadata/pcawg_sample_sheet.tsv",
        icgc="data/pcawg/any___cancer_icgc___donor_ids.txt",
        tcga="data/pcawg/any___cancer_tcga___donor_ids.txt"
    output:
        "metadata/pcawg_metadata.csv"
    resources:
        mem=4000
    shell:
        "snakemake/scripts/make_pcawg_metadata.R {input.ss} {input.icgc} {input.tcga} {output}"


# Allow script to run (and presumably create this file) by specifying
# --config make_pcawg_metadata=1 --until metadata/pcawg_metadata.csv
if 'make_pcawg_metadata' not in config.keys():
    pcawg_meta = pd.read_csv('metadata/pcawg_metadata.csv')
    # make a donor_id -> {tcga|icgc} mapping
    proj_map = dict(zip(list(pcawg_meta['icgc_donor_id']), list(pcawg_meta['project'])))
    pcawg_tumors = sorted(set(pcawg_meta['tumor']))

rule pcawg_extract_sample_vcf:
    input:
        lambda wildcards: "input/any___cancer_" + proj_map[wildcards.donor] + '___none.maf'
    output:
        maf=temp("data/pcawg/sample_vcfs/{donor}.maf"),
        pairs=temp("data/pcawg/sample_vcfs/{donor}.pairs.tsv"),
        vcf="data/pcawg/sample_vcfs/{donor}.vcf"
    log:
        "data/pcawg/sample_vcfs/{donor}.log"
    benchmark:
        "data/pcawg/sample_vcfs/{donor}.benchmark.txt"
    params:
        donor_id='{donor}'
    resources:
        mem=4000
    shell:
        """
        snakemake/scripts/pcawg_extract_one_maf.sh \
            {params.donor_id} \
            {input} \
            {output.maf} \
            data/pcawg/sample_vcfs
        """


rule pcawg_make_tumor_vcf:
    input:
        vcfs=lambda wildcards: list(pcawg_meta.loc[(pcawg_meta.tumor == wildcards.tumor) & (pcawg_meta.maf_processing_error == False), 'file'])
    output:
        "data/pcawg/tumor_vcfs/{tumor}.vcf"
    resources:
        # needs the entire VCF in RAM for sort
        mem=8000
    params:
        tumor="{tumor}"
    shell:
        """
        snakemake/scripts/make_pcawg_tumor_vcf.sh {params.tumor} {input} > {output}
        """


rule pcawg_snpeff:
    input:
        "data/pcawg/tumor_vcfs/{tumor}.vcf"
    output:
        "data/pcawg/snpeff_vcfs/{tumor}.snpeff.vcf"
    log:
        "data/pcawg/snpeff_vcfs/{tumor}.snpeff.log"
    resources:
        mem=8000
    shell:
        """
        snakemake/scripts/run_snpeff_pcawg.sh {input} {output} > {log} 2>&1
        """


# Same as above, but input format is different. Also add some extra VCF
# tags to the normal cell VCFs.
ruleorder: pcawg_snpeff_singlecell > pcawg_snpeff
rule pcawg_snpeff_singlecell:
    input:
        "vcfs/{celltype}___mut___{qualtype}.vcf"
    output:
        tmp=temp("data/pcawg/snpeff_vcfs/{celltype}___{qualtype}.snpeff.vcf.tmp"),
        vcf="data/pcawg/snpeff_vcfs/{celltype}___{qualtype}.snpeff.vcf"
    log:
        "data/pcawg/snpeff_vcfs/{celltype}___{qualtype}.snpeff.log"
    params:
        celltype='{celltype}'
    resources:
        mem=8000
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ $8 = "TType={params.celltype};Origin=.;Sample=."; print $0 }}' {input} > {output.tmp} \
            | snakemake/scripts/run_snpeff_pcawg.sh {output.tmp} {output.vcf} > {log} 2>&1
        """


rule pcawg_parse_snpeff:
    input:
        "data/pcawg/snpeff_vcfs/{tumor_or_celltype}.snpeff.vcf"
    output:
        "data/pcawg/{tumor_or_celltype}.gene_mutations.txt"
    resources:
        mem=8000
    shell:
        """
        snakemake/scripts/parse_pcawg_snpeff.sh {input} > {output}
        """


'''
ruleorder: parse_singlecell_snpeff > pcawg_parse_snpeff
rule parse_singlecell_snpeff:
    input:
        "data/pcawg/snpeff_vcfs/{celltype}___{qualtype}.snpeff.vcf"
    output:
        "data/pcawg/{celltype}___{qualtype}.gene_mutations.txt"
    params:
        celltype='{celltype}'
    resources:
        mem=8000
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ $8 = "TType={params.celltype};Origin=.;Sample=.;" $8; print $0 }}' {input} \
            | snakemake/scripts/parse_pcawg_snpeff.sh /dev/stdin > {output}
        """
'''


rule pcawg_gene_mutation_odds_ratios:
    input:
        neuron="data/pcawg/neuron___AB.gene_mutations.txt",
        oligo="data/pcawg/oligo___AB.gene_mutations.txt",
        tumor="data/pcawg/{tumor}.gene_mutations.txt"
    output:
        rda="data/pcawg/{tumor}.gene_mutation_odds_ratios.rda"
    log:
        "data/pcawg/{tumor}.gene_mutation_odds_ratios.log"
    resources:
        mem=8000
    shell:
        """
        snakemake/scripts/pcawg_gene_odds_ratio.R \
            {input.tumor} {input.neuron} {input.oligo} {output.rda} > {log} 2>&1
        """
