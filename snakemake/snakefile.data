# vim: syntax=python
# Manage copying and performing trivial format conversions of local data
# and downloading external datasets.


# manifest is a pandas data table of all input files + metadata
# # for now, we assume the concatenation of all metadata produces
# # a unique ID for each file.
def make_local_mapping(manifest, namesep="___"):
    manifest_size = len(manifest.index)
    local_name = manifest['celltype'] + namesep + manifest['filetype'] + namesep + manifest['qualtype'] + '.' + manifest['fileext']
    out_to_in = dict(zip(local_name, manifest['filepath']))
    dict_size = len(out_to_in)
    # make sure each line in maniest has one line in the dict -
    # equivalent to making sure the metadata concatenation is unique
    if manifest_size != dict_size:
        raise RuntimeError('metadata in input manifest does not uniquely identify files')
    
    return(out_to_in)


input_copy_mapping = make_local_mapping(manifest)
rule copy_inputs:
    input:
        lambda wildcards: input_copy_mapping[wildcards.celltype + "___" + wildcards.filetype + "___" + wildcards.qualtype + "." + wildcards.fileext]
    resources:
        mem=1000
    output:
        "input/{celltype}___{filetype}___{qualtype}.{fileext}"
    shell:
        """
        cp -n {input} {output}
        """


# Create passA, passAB, etc. RData files from CSV files.
# Results used to be provided in RDA format. Now it's somewhat counterproductive
# to do this but it isn't worth reworking the entire pipeline.
rule mut_csv_to_rda:
    input:
        "scan2_output/{celltype}_snv_indel_pass_rescue.txt"
    output:
        "input/{celltype}___mut___{qualtype}.rda"
    params:
        muttype=lambda wildcards: 'indel' if wildcards.qualtype.startswith('indel') else 'snv',
        rescue=lambda wildcards: '| rescue == TRUE' if wildcards.qualtype.endswith("AB") else ''
    resources:
        mem=1000
    shell:
        """
        Rscript -e 'library(data.table); muts <- fread("{input}"); muts <- muts[muttype == "{params.muttype}" & (pass == TRUE {params.rescue})]; save(muts, file="{output}")'
        """


rule tables_rda_to_csv:
    input:
        "input/{celltype}___{filetype}___{qualtype}.rda",
    output:
        "tables/{celltype}___{filetype}___{qualtype}.csv",
    resources:
        mem=4000
    log:
        "tables/{celltype}___{filetype}___{qualtype}.log"
    script:
        "scripts/rda_to_csv.R"


rule rda_to_vcf:
    input:
        "input/{celltype}___mut___{qualtype}.rda"
    output:
        "vcfs/{celltype}___mut___{qualtype}.vcf"
    resources:
        mem=4000
    log:
        "vcfs/{celltype}___mut___{qualtype}.rda_to_vcf.log"
    script:
        "scripts/rda_to_vcf.R"


rule download_roadmap_narrowpeak:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{wildcards.eid}-{wildcards.mark}.narrowPeak.gz"
        """


rule download_roadmap_bigwig:
    input:
    output:
        "data/roadmap/histone_marks/bigwig/{eid}-{mark}.fc.signal.bigwig"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/signal/consolidated/macs2signal/foldChange/{wildcards.eid}-{wildcards.mark}.fc.signal.bigwig"
        """


rule download_encode_replichip_bigwig:
    input:
    output:
        "data/encode/replichip/bigwig/{enc_id}.bigwig"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://www.encodeproject.org/files/{wildcards.enc_id}/@@download/{wildcards.enc_id}.bigWig"
        """


rule make_gtex_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="input/any___gtex_expression_tpm___none.gct"
    output:
        bigwig="data/gtex/bigwig/{tissue}.bigwig"
    log:
        "data/gtex/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


rule map_scrnaseq_to_gtex_gene_model:
    input:
        scrnaseq='input/any___scrnaseq_mean_expression___none.csv',
        gct='input/any___gtex_expression_tpm___none.gct'
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_matrix.log"
    resources:
        mem=16000
    script:
        "scripts/map_scrnaseq_to_gtex_gene_model.R"


rule combine_scrnaseq_by_celltype:
    input:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.log"
    resources:
        mem=8000
    script:
        "scripts/combine_scrna_by_celltype.R"


def get_scrnaseq_matrix_file(wildcards):
    # This hack is to enable the combined expression profiles per cell
    # type across the several scRNAseq libraries.
    # Could be done in a better way.
    if wildcards.tissue in [ 'Astrocytes', 'Microglia', 'Endothelial', 'Oligodendrocytes', 'OPCs', 'Neurons', 'Inhibitory-Neurons', 'Excitatory-Neurons']:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    else:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"

    return({ "gtf": "input/any___gtex_gene_model___none.gtf", "gct": gct })


rule make_scrnaseq_signal_bigwig:
    input:
        #gtf="input/any___gtex_gene_model___none.gtf",
        #gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
        unpack(get_scrnaseq_matrix_file)
    output:
        bigwig="data/scrnaseq/expression/bigwig/{tissue}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"



# Exactly the same as above rule, just using a different input gct
# Probably a better way to do this.
rule make_scrnaseq_by_celltype_signal_bigwig:
    input:
        gtf="input/any___gtex_gene_model___none.gtf",
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    output:
        bigwig="data/scrnaseq/expression/bigwig/{celltype}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{celltype}.log"
    params:
        tissue="{celltype}"
    resources:
        mem=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


methyltype_to_path = {
    'WGBS_FractionalMethylation' : 'WGBS/FractionalMethylation',
    'WGBS_ReadCoverage' : 'WGBS/ReadCoverage',
    'RRBS_FractionalMethylation' : 'RRBS/FractionalMethylation',
    'RRBS_ReadCoverage' : 'RRBS/ReadCoverage'
}

rule download_roadmap_dnamethyl:
    input:
    output:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.bigwig"
    params:
        path=lambda wildcards: methyltype_to_path[wildcards.methyltype]
    log:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.log"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byDataType/dnamethylation/{params.path}_bigwig/$outfile"
        """


# Both downloads the data and splits it back into its original 200bp bins.
# By using a standard bin size, all 100+ epigenomes can be analyzed by a
# single bedenrich run.
# The alignability tiles here are ONLY intended to split the data into its
# original 200bp bins - the BEDs are still filtered afterward for 100bp
# alignability.
rule download_roadmap_chromhmm15:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/15state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed"),
    resources:
        mem=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/coreMarks/jointModel/final/{wildcards.eid}_15_coreMarks_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_chromhmm18:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/18state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed"),
    resources:
        mem=4000
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/core_K27ac/jointModel/final/{wildcards.eid}_18_core_K27ac_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_histone_peaks:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{eid}-{mark}.narrowPeak.gz
        """


rule download_icgc_muts:
    input:
    output:
        "data/icgc/final_consensus_passonly.snv_mnv_indel.icgc.public.maf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz \
            "https://dcc.icgc.org/api/v1/download?fn=/PCAWG/consensus_snv_indel/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
        gunzip -c {output}.gz > {output}
        """


rule split_cancer_muts_by_type:
    input:
        tcga="input/any___cancer_tcga___none.maf",
        icgc="input/any___cancer_icgc___none.maf"
    output:
        "data/cancer_snvdens/snvs/{tumor}.txt"
    params:
        tumor="{tumor}"
    resources:
        mem=1000
    shell:
        """
        (echo "chr,pos,refnt,altnt,sample"|tr ',' '\t' ;
         cat {input.tcga} {input.icgc} | cut -f 2,3,4,7,8,10,42,43 | grep "{params.tumor}" | grep SNP | cut -f1,2,5,6,8) > {output}
        """


rule make_cancer_snvdens_bigwigs:
    input:
        txt="data/cancer_snvdens/snvs/{tissue}.txt"
    output:
        sumbigwig="data/cancer_snvdens/bigwig/{tissue}_sumdens.bigwig",
        normbigwig="data/cancer_snvdens/bigwig/{tissue}_normdens.bigwig"
    log:
        "data/cancer_snvdens/bigwig/{tissue}.log"
    benchmark:
        "data/cancer_snvdens/bigwig/{tissue}.benchmark.txt"
    resources:
        # this extreme memory req is because a 100bp bin x #samples matrix is built
        # cancer types with 200+ samples fail at 96G and I don't know how much memory is truly needed
        # so this very large number is to avoid yet another failure.
        #mem=lambda wildcards, attempt: 32000 if attempt==1 else 196000
        mem=196000
    script:
        "scripts/make_cancer_mutdens_bigwigs.R"


rule download_gencode_genes:
    input:
    output:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz "https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v26lift37.annotation.gtf.gz"
        gunzip -c {output}.gz > {output}
        """



rule make_gtex_gene_model:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        script="data/gtex/collapse_annotation.py",
        gtf="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.gtf",
        gtf_genes="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    resources:
        mem=1000
    shell:
        """
        wget -O {output.script} \
            "https://raw.githubusercontent.com/broadinstitute/gtex-pipeline/master/gene_model/collapse_annotation.py"
        python {output.script} {input} {output.gtf}
        awk '$0 ~ /^#/ || $3 == "gene"' {output.gtf} > {output.gtf_genes}
        """



rule download_nott_bigwigs:
    input:
    output:
        "data/nott/bigwig/{file}"
    params:
        file="{file}",
        mark=lambda wildcards: 'atac' if 'atac' in wildcards.file else ('h3k27ac' if 'H3K27ac' in wildcards.file else 'h3k4me3')
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "http://homer.ucsd.edu/hubs/nuclei_{params.mark}_hg19_pooled/hg19/{params.file}"
        """



rule download_gtex_expression:
    input:
    output:
        "data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    resources:
        mem=1000
    shell:
        """
        wget -O {output}.gz \
            "https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz"
        gunzip -c {output}.gz > {output}
        """


rule download_ucsc_conservation:
    input:
    output:
        phastcons="data/ucsc/conservation/hg19.100way.phastCons.bw",
        phylop="data/ucsc/conservation/hg19.100way.phyloP100way.bw"
    resources:
        mem=1000
    shell:
        """
        wget -O {output.phylop} "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/phyloP100way/hg19.100way.phyloP100way.bw"
        wget -O {output.phastcons} "https://hgdownload.cse.ucsc.edu/goldenPath/hg19/phastCons100way/hg19.100way.phastCons.bw"
        """



rule download_repliseq:
    input:
    output:
        "data/repliseq/bigwig/{file}"
    params:
        file="{file}"
    resources:
        mem=1000
    shell:
        """
        wget -O {output} \
            "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeUwRepliSeq/{params.file}"
        """



#rule make_depth_bigwig:
    #input:
    #output:
        #"data/depth/bigwig/
