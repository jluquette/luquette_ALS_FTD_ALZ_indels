# vim: syntax=python
# Manage copying and performing trivial format conversions of local data
# and downloading external datasets.


rule download_roadmap_narrowpeak:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem_mb=1000,
        roadmap_download=1
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{wildcards.eid}-{wildcards.mark}.narrowPeak.gz"
        """


rule download_roadmap_bigwig:
    input:
    output:
        "data/roadmap/histone_marks/bigwig/{eid}-{mark}.fc.signal.bigwig"
    resources:
        mem_mb=1000,
        roadmap_download=1
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/signal/consolidated/macs2signal/foldChange/{wildcards.eid}-{wildcards.mark}.fc.signal.bigwig"
        """


rule download_encode_replichip_bigwig:
    input:
    output:
        "data/encode/replichip/bigwig/{enc_id}.bigwig"
    resources:
        mem_mb=1000,
        encode_download=1
    shell:
        """
        wget -O {output} \
            "https://www.encodeproject.org/files/{wildcards.enc_id}/@@download/{wildcards.enc_id}.bigWig"
        """


rule make_gtex_signal_bigwig:
    input:
        gtf="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf",
        gct="data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    output:
        bigwig="data/gtex/bigwig/{tissue}.bigwig"
    log:
        "data/gtex/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem_mb=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


rule convert_seurat_object_to_table:
    input:
        rds='scrnaseq/{scrnaseq_object}.rds'
    output:
        csv='scrnaseq/{scrnaseq_object}_table.csv'
    log:
        'scrnaseq/{scrnaseq_object}_table.log'
    benchmark:
        'scrnaseq/{scrnaseq_object}_table.benchmark.txt'
    resources:
        mem_mb=14000
    script:
        "scripts/seurat_rds_to_csv.R"


rule map_scrnaseq_to_gtex_gene_model:
    input:
        scrnaseq='scrnaseq/combined_mean_expr.csv',
        gct="data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_matrix.log"
    resources:
        mem_mb=16000
    script:
        "scripts/map_scrnaseq_to_gtex_gene_model.R"


rule combine_scrnaseq_by_celltype:
    input:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"
    output:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    log:
        "data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.log"
    resources:
        mem_mb=8000
    script:
        "scripts/combine_scrna_by_celltype.R"


def get_scrnaseq_matrix_file(wildcards):
    # This hack is to enable the combined expression profiles per cell
    # type across the several scRNAseq libraries.
    # Could be done in a better way.
    if wildcards.tissue in [ 'Astrocytes', 'Microglia', 'Endothelial', 'Oligodendrocytes', 'OPCs', 'Neurons', 'Inhibitory-Neurons', 'Excitatory-Neurons']:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_by_celltype_matrix.gct"
    else:
        gct="data/scrnaseq/expression/scrnaseq_mean_expression_matrix.gct"

    return({ "gtf": "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf", "gct": gct })


rule make_scrnaseq_signal_bigwig:
    input:
        unpack(get_scrnaseq_matrix_file)
    output:
        bigwig="data/scrnaseq/expression/bigwig/{tissue}.bigwig"
    log:
        "data/scrnaseq/expression/bigwig/{tissue}.log"
    params:
        tissue="{tissue}"
    resources:
        mem_mb=16000
    script:
        "scripts/convert_gtex_expression_to_bigwig.R"


methyltype_to_path = {
    'WGBS_FractionalMethylation' : 'WGBS/FractionalMethylation',
    'WGBS_ReadCoverage' : 'WGBS/ReadCoverage',
    'RRBS_FractionalMethylation' : 'RRBS/FractionalMethylation',
    'RRBS_ReadCoverage' : 'RRBS/ReadCoverage'
}

rule download_roadmap_dnamethyl:
    input:
    output:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.bigwig"
    params:
        path=lambda wildcards: methyltype_to_path[wildcards.methyltype]
    log:
        "data/roadmap/dnamethyl/{methyltype}/bigwig/{enc_id}_{methyltype}.log"
    resources:
        mem_mb=1000,
        roadmap_download=1
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byDataType/dnamethylation/{params.path}_bigwig/$outfile"
        """


# Both downloads the data and splits it back into its original 200bp bins.
# By using a standard bin size, all 100+ epigenomes can be analyzed by a
# single bedenrich run.
# The alignability tiles here are ONLY intended to split the data into its
# original 200bp bins - the BEDs are still filtered afterward for 100bp
# alignability.
rule download_roadmap_chromhmm15:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/15state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/15state/bed/{eid}.bed"),
    resources:
        mem_mb=4000,
        roadmap_download=1
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/coreMarks/jointModel/final/{wildcards.eid}_15_coreMarks_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_chromhmm18:
    input:
        tiles="alignability/genome_tiles/genome_tiles_200binsize.bed"
    output:
        bed="data/roadmap/chromhmm/18state/bed/{eid}___200bp_tiles.bed",
        tmp1=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed.gz"),
        tmp2=temp("data/roadmap/chromhmm/18state/bed/{eid}.bed"),
    resources:
        mem_mb=4000,
        roadmap_download=1
    shell:
        """
        wget -O {output.tmp1} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/core_K27ac/jointModel/final/{wildcards.eid}_18_core_K27ac_mnemonics.bed.gz"
        gunzip -c {output.tmp1} > {output.tmp2}
        bedtools intersect -loj -a {input.tiles} -b {output.tmp2} | cut -f1-3,10 > {output.bed}
        """


rule download_roadmap_histone_peaks:
    input:
    output:
        "data/roadmap/histone_marks/narrowPeak/{eid}-{mark}.narrowPeak"
    resources:
        mem_mb=1000,
        roadmap_download=1
    shell:
        """
        wget -O {output} \
            "https://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/{eid}-{mark}.narrowPeak.gz
        """


rule download_pcawg_muts:
    input:
    output:
        "data/pcawg/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "https://dcc.icgc.org/api/v1/download?fn=/PCAWG/consensus_snv_indel/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
        """


rule split_cancer_muts_by_type:
    input:
        tcga="data/pcawg/cancer_tcga.maf",
        icgc="data/pcawg/cancer_icgc.maf"
    output:
        "data/cancer_snvdens/snvs/{tumor}.txt"
    params:
        tumor="{tumor}"
    resources:
        mem_mb=1000
    shell:
        """
        (echo "chr,pos,refnt,altnt,sample"|tr ',' '\t' ;
         cat {input.tcga} {input.icgc} | cut -f 2,3,4,7,8,10,42,43 | grep "{params.tumor}" | grep SNP | cut -f1,2,5,6,8) > {output}
        """


rule make_cancer_snvdens_bigwigs:
    input:
        txt="data/cancer_snvdens/snvs/{tissue}.txt"
    output:
        sumbigwig="data/cancer_snvdens/bigwig/{tissue}_sumdens.bigwig",
        normbigwig="data/cancer_snvdens/bigwig/{tissue}_normdens.bigwig"
    log:
        "data/cancer_snvdens/bigwig/{tissue}.log"
    benchmark:
        "data/cancer_snvdens/bigwig/{tissue}.benchmark.txt"
    resources:
        # this extreme memory req is because a 100bp bin x #samples matrix is built
        # cancer types with 200+ samples fail at 96G and I don't know how much memory is truly needed
        # so this very large number is to avoid yet another failure.
        #mem_mb=lambda wildcards, attempt: 32000 if attempt==1 else 196000
        mem_mb=196000
    script:
        "scripts/make_cancer_mutdens_bigwigs.R"


rule download_gencode_genes:
    input:
    output:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output}.gz "https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v26lift37.annotation.gtf.gz"
        gunzip -c {output}.gz > {output}
        """


rule make_gtex_gene_model:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        script="data/gtex/collapse_annotation.py",
        gtf="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.gtf",
        gtf_genes="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    resources:
        mem_mb=4000
    shell:
        """
        wget -O {output.script} \
            "https://raw.githubusercontent.com/broadinstitute/gtex-pipeline/master/gene_model/collapse_annotation.py"
        python {output.script} {input} {output.gtf}
        awk '$0 ~ /^#/ || $3 == "gene"' {output.gtf} > {output.gtf_genes}
        """


rule download_nott_bigwigs:
    input:
    output:
        "data/nott/bigwig/{file}"
    params:
        file="{file}",
        mark=lambda wildcards: 'atac' if 'atac' in wildcards.file else ('h3k27ac' if 'H3K27ac' in wildcards.file else 'h3k4me3')
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output} \
            "http://homer.ucsd.edu/hubs/nuclei_{params.mark}_hg19_pooled/hg19/{params.file}"
        """


ruleorder: copyover_nott_supptab5 > download_nott_supptab5
rule copyover_nott_supptab5:
    input:
        "external_data/aay0793-nott-table-s5.xlsx"
    output:
        "data/nott/SupplementaryTable5.xlsx"
    resources:
        mem_mb=4000  # failed once at 100mb
    shell:
        "cp -n {input} {output}"


rule download_nott_supptab5:
    input:
    output:
        "data/nott/SupplementaryTable5.xlsx"
    resources:
        mem_mb=1000
    message:
        "This rule will always fail because science.org blocks simple wget downloads. Please open `https://www.science.org/doi/suppl/10.1126/science.aay0793/suppl_file/aay0793-nott-table-s5.xlsx` in a web browser and place the file in data/nott/SupplementaryTable5.xlsx"
    shell:
        """
        wget -O {output} \
            "https://www.science.org/doi/suppl/10.1126/science.aay0793/suppl_file/aay0793-nott-table-s5.xlsx"
        """


rule parse_nott_supptab5:
    input:
        xlsx="data/nott/SupplementaryTable5.xlsx"
    output:
        temp(expand('data/nott/bed/{celltype}_{regiontype}.csv',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ])),
        expand('data/nott/bed/{celltype}_{regiontype}.bed',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ])
    log:
        'data/nott/bed/parse_nott_supptab5.log'
    params:
        out_prefix='data/nott/bed'
    resources:
        mem_mb=1000
    script:
        'scripts/parse_nott2019_supptable.R'


rule make_final_nott_bed:
    input:
        beds=expand('data/nott/bed/{celltype}_{regiontype}.bed',
            celltype=[ 'astrocyte', 'neuron', 'oligo', 'microglia' ],
            regiontype=[ 'enhancer', 'promoter' ]), 
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        'data/nott/bed/enhancers_and_promoters_all_celltypes.bed'
    resources:
        mem_mb=1000
    shell:
        """
        cat {input.beds} | bedtools sort -g {input.genome_ordering} > {output}
        """


rule download_gtex_expression:
    input:
    output:
        "data/gtex/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output}.gz \
            "https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz"
        gunzip -c {output}.gz > {output}
        """


rule download_ucsc_conservation:
    input:
    output:
        phastcons="data/ucsc/conservation/hg19.100way.phastCons.bw",
        phylop="data/ucsc/conservation/hg19.100way.phyloP100way.bw"
    resources:
        mem_mb=1000,
        ucsc_download=1
    shell:
        """
        wget -O {output.phylop} "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/phyloP100way/hg19.100way.phyloP100way.bw"
        wget -O {output.phastcons} "https://hgdownload.cse.ucsc.edu/goldenPath/hg19/phastCons100way/hg19.100way.phastCons.bw"
        """


rule download_repliseq:
    input:
    output:
        "data/repliseq/bigwig/{file}"
    params:
        file="{file}"
    resources:
        mem_mb=1000,
        ucsc_download=1
    shell:
        """
        wget -O {output} \
            "http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeUwRepliSeq/{params.file}"
        """


# Both copy the MAF file and remove carriage returns because maf2vcf.pl
# can't handle them.
rule pcawg_copy_pcawg_maf:
    input:
        "data/pcawg/final_consensus_passonly.snv_mnv_indel.icgc.public.maf.gz"
    output:
        "data/pcawg/cancer_icgc.maf"
    resources:
        mem_mb=1000
    shell:
        """
        gunzip -c {input} | tr -d '\r' > {output}
        """


rule pcawg_copy_tcga_maf:
    input:
        "data/pcawg/final_consensus_passonly.snv_mnv_indel.tcga.controlled.maf.gz"
    output:
        "data/pcawg/cancer_tcga.maf"
    resources:
        mem_mb=1000
    shell:
        """
        gunzip -c {input} | tr -d '\r' > {output}
        """


rule pcawg_get_donor_ids:
    input:
        "data/pcawg/cancer_{cancer_project}.maf",
    output:
        "data/pcawg/cancer_{cancer_project}___donor_ids.txt"
    resources:
        mem_mb=1000
    shell:
        """
        ( head -1 {input} | cut -f42,43 ;
          tail -n +2 {input} | cut -f42,43 | uniq | sort | uniq)  > {output}
        """


rule pcawg_make_metadata:
    input:
        ss="metadata/pcawg_sample_sheet.tsv",
        icgc="data/pcawg/any___cancer_icgc___donor_ids.txt",
        tcga="data/pcawg/any___cancer_tcga___donor_ids.txt"
    output:
        "metadata/pcawg_metadata.csv"
    resources:
        mem_mb=4000
    shell:
        "snakemake/scripts/make_pcawg_metadata.R {input.ss} {input.icgc} {input.tcga} {output}"


rule pcawg_extract_sample_vcf:
    input:
        lambda wildcards: "data/pcawg/cancer_" + config['pcawg_proj_map'][wildcards.donor] + '.maf'
    output:
        maf=temp("data/pcawg/sample_vcfs/{donor}.maf"),
        pairs=temp("data/pcawg/sample_vcfs/{donor}.pairs.tsv"),
        vcf="data/pcawg/sample_vcfs/{donor}.vcf"
    log:
        "data/pcawg/sample_vcfs/{donor}.log"
    benchmark:
        "data/pcawg/sample_vcfs/{donor}.benchmark.txt"
    params:
        donor_id='{donor}'
    resources:
        mem_mb=4000
    shell:
        """
        snakemake/scripts/pcawg_extract_one_maf.sh \
            {params.donor_id} \
            {input} \
            {output.maf} \
            data/pcawg/sample_vcfs
        """


rule pcawg_make_tumor_vcf:
    input:
        vcfs=lambda wildcards: list(config['pcawg_metadata'].loc[(config['pcawg_metadata'].tumor == wildcards.tumor) & (config['pcawg_metadata'].maf_processing_error == False), 'file'])
    output:
        "data/pcawg/cancer_vcfs/{tumor}.vcf"
    resources:
        # needs the entire VCF in RAM for sort
        mem_mb=8000
    params:
        tumor="{tumor}"
    shell:
        """
        snakemake/scripts/make_pcawg_cancer_vcf.sh {params.tumor} {input} > {output}
        """


rule make_gencode_gene_regions:
    input:
        "data/gencode/gencode.v26lift37.annotation.gtf"
    output:
        "data/gencode/gene_regions.bed"
    resources:
        mem_mb=8000
    shell:
        "snakemake/scripts/make_gene_regions.R {input} {output}"


rule simplify_gencode_gene_regions:
    input:
        "data/gencode/gene_regions.bed"
    output:
        "data/gencode/gene_regions_simplified.bed"
    localrule: True
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        sed -e 's/upstream$/intergenic/g' \
            -e 's/downstream$/intergenic/g' \
            -e 's/cds$/exonic/g' \
            -e 's/utr5$/exonic/g' \
            -e 's/utr3$/exonic/g' \
            -e 's/intron$/intronic/g' {input} > {output}
        """

# The two previous gencode gene regions were made to allow more fine grained
# access to UTRs, introns and exons. But:
#   1. those regions are too small for sparse mutation types like indels
#   2. we only make a claim about transcribed genome regions; whether they
#      contribute to the final protein is not currently useful.
#   3. classifying the genome from the full GTF is messy.
rule make_gencode_transcribed_regions:
    input:
        bed="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.bed",
        genome="resources/hg19.withXYM.chrprefix.genome"
    output:
        tx=temp("data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.tx.bed"),
        utx=temp("data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.utx.bed"),
        bed="data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.tx_or_utx.bed"
    log:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.tx_or_utx.log"
    localrule: True
    resources:
        mem_mb=4000,
        localjob=1
    shell:
        # important: bedtools merge to convert overlapping gene records into a single
        # transcribed region record, or else there will be double counting.
        """
        bedtools sort -i {input.bed} -g {input.genome} \
            | bedtools merge -i /dev/stdin \
            | awk 'BEGIN {{OFS="\t";}} {{ print $1, $2+1, $3, "tx"; }}' > {output.tx}
        bedtools sort -i {input.bed} -g {input.genome} \
            | bedtools complement -i /dev/stdin -g {input.genome} \
            | awk 'BEGIN {{OFS="\t";}} {{ print $1, $2+1, $3, "utx"; }}' > {output.utx}
        cat {output.tx} {output.utx} | bedtools sort -i /dev/stdin -g {input.genome} > {output.bed}
        """


rule make_gencode_gene_bed:
    input:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    output:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.bed"
    log:
        "data/gtex/make_gencode_gene_bed.log"
    localrule: True
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ if ($1 ~ /chr[0-9]+/) {{ match($0, /gene_name [^;]*/);  print $1, $4, $5, substr($0, RSTART, RLENGTH); }} }}' {input} \
            | sed -e 's/gene_name //' \
            | tr -d '"' > {output}
        """


rule make_gencode_gene_bed_protein_coding:
    input:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.gtf"
    output:
        "data/gtex/gencode.v26lift37.annotation.GTEX_COLLAPSED.genes_only.protein_coding.bed"
    log:
        "data/gtex/make_gencode_gene_bed.protein_coding.log"
    localrule: True
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        awk 'BEGIN {{ OFS="\t"; }} {{ if ($1 ~ /chr[0-9]+/ && $0 ~ /gene_type "protein_coding";/) {{ match($0, /gene_name [^;]*/);  print $1, $4, $5, substr($0, RSTART, RLENGTH); }} }}' {input} \
            | sed -e 's/gene_name //' \
            | tr -d '"' > {output}
        """


'''
ruleorder: subset_by_mutsig > copy_inputs
rule subset_by_mutsig:
    input:
        rda="input/{celltype}___{filetype}___{qualtype}.{fileext}"
    output:
        rda="input/{celltype}_{sig}___{filetype}___{qualtype}.{fileext}"
    log:
        "input/{celltype}_{sig}___{filetype}___{qualtype}.{fileext}.log"
    params:
        sig='{sig}'
    resources:
        mem_mb=12000
    script:
        "scripts/subset_by_mutsig.R"
'''


rule machado2022_sbsblood_wget:
    input:
    output:
        "data/machado2022_sbsblood/Machado2022_SupplementaryTables_All.xlsx"
    log:
        "data/machado2022_sbsblood/wget.log"
    localrule: True
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        wget -O {output} \
            'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9402440/bin/41586_2022_5072_MOESM4_ESM.xlsx'
        """


rule machado2022_sbsblood_convert:
    input:
        xlsx="data/machado2022_sbsblood/Machado2022_SupplementaryTables_All.xlsx"
    output:
        supptab8="data/machado2022_sbsblood/SupplementaryTable8.csv",
        csv="data/machado2022_sbsblood/SBSblood.csv",
        pdf="data/machado2022_sbsblood/SBSblood.pdf",
        svg="data/machado2022_sbsblood/SBSblood.svg"
    log:
        "data/machado2022_sbsblood/convert.log"
    params:
        sheet_number=8
    localrule: True
    resources:
        mem_mb=1000,
        localjob=1
    script:
        "scripts/get_machado2022_sbsblood.R"


rule leesix2018_hspc_spectrum_get:
    input:
    output:
        "data/leesix2018_hspc/All_mutations_SBS_indel.txt"
    log:
        "data/leesix2018_hspc/wget.log"
    localrule: True
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        wget -O {output} \
            'https://data.mendeley.com/public-files/datasets/yzjw2stk7f/files/46faf00b-02de-44e7-a5ea-bf84e14e875e/file_downloaded'
        """


rule leesix2018_hspc_spectrum_convert:
    input:
        txt="data/leesix2018_hspc/All_mutations_SBS_indel.txt"
    output:
        csv="data/leesix2018_hspc/HSPC_spectrum.csv",
        pdf="data/leesix2018_hspc/HSPC_spectrum.pdf",
        svg="data/leesix2018_hspc/HSPC_spectrum.svg",
    log:
        "data/leesix2018_hspc/convert.log"
    localrule: True
    resources:
        mem_mb=1000,
        localjob=1
    script:
        "scripts/get_leesix2018_hspc.R"


rule copyover_boca2_peaks:
    input:
        GABA="external_data/GABA_DLPFC.bed.gz",
        GLU="external_data/GLU_DLPFC.bed.gz",
        OLIG="external_data/OLIG_DLPFC.bed.gz",
        MGAS="external_data/MGAS_DLPFC.bed.gz",
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        "data/boca2/OCRs_DLPFC.bed"
    resources:
        mem_mb=1000
    shell:
        """
        (gunzip -c {input.GABA} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "GABA"; }}';
         gunzip -c {input.GLU} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "GLU"; }}';
         gunzip -c {input.OLIG} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "OLIG"; }}';
         gunzip -c {input.MGAS} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "MGAS"; }}') \
        | bedtools sort -g {input.genome_ordering} > {output}
        """


rule download_sarseq_hotspots:
    input:
    output:
        gzbed=temp("data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed.gz"),
        bed="data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed"
    resources:
        mem_mb=1000
    shell:
        """
        wget -O {output.gzbed} \
            "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE167257&format=file&file=GSE167257%5FSARseq%5FiNeuron%5FOverlapRep123%2Epeaks%2Ebed%2Egz"
        gunzip -c {output.gzbed} | awk 'BEGIN {{ OFS="\t"; }} {{ print $0, "SAR-seq"; }}' > {output.bed}
        """


rule copyover_repairseq_supptab:
    input:
        "external_data/abb9032_Table_S1.xlsx"
    output:
        "data/dna_repair_hotspots/Reid2021_SupplementaryTable1.xlsx"
    resources:
        mem_mb=1000
    shell:
        "cp -n {input} {output}"


rule parse_repairseq_supptab:
    input:
        xlsx="data/dna_repair_hotspots/Reid2021_SupplementaryTable1.xlsx"
    output:
        csv=temp("data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.csv"),
        bed="data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.bed",
    log:
        'data/nott/dna_repair_hotspots/parse_reid2021_supptable.log'
    resources:
        mem_mb=1000
    params:
        out_prefix='data/dna_repair_hotspots'
    script:
        'scripts/parse_reid2021_supptable.R'


rule make_final_dna_repair_hotspot_bed:
    input:
        repairseq="data/dna_repair_hotspots/Reid2021_RepairSeq_hotspots.bed",
        sarseq="data/dna_repair_hotspots/Wu2021_SARseq_peaks_overlap.bed",
        genome_ordering='snakemake/scripts/hg19.chrom.sizes'
    output:
        "data/dna_repair_hotspots/DNArepair_hotspots.bed"
    resources:
        mem_mb=1000
    shell:
        """
        cat {input.repairseq} {input.sarseq} \
            | bedtools sort -g {input.genome_ordering} > {output}
        """
