# vim: syntax=python

bams = config['bams']
all_bams = config['all_bams']


# I don't know how to check the git commit ID of the installed R package (or
# how to condition a snakemake rule on a commit ID rather than an output file).
# So this rule would not be capable of keeping the R version updated, but will
# work once. It's mostly here for posterity.
rule download_specific_scan2_rpkg_version:
    input:
    output:
        # The real result of this rule is installing the R package. This directory
        # just serves as a flawed way of forcing this rule to run in roughly the
        # right dependency order.
        dir=directory('scan2/r-scan2_specific_commit')
    log:
    params:
        commit_id=config['scan2_version']['r-scan2']['installed_commit_id']
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        # The git clone does not actually provide the r-scan2 package.  It merely
        # imports the source at the correct commit ID as a reference.  The install_github
        # call actually installs the R package directly from github.  The reason we use
        # devtools' install_github() is to inject the commit ID into the installed R package's
        # version data, enabling version tracking.
        """
        rm -fr {output.dir}
        git clone https://github.com/parklab/r-scan2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        Rscript -e 'devtools::install_github("parklab/r-scan2@{params.commit_id}", upgrade="never")'
        """


rule download_specific_scan2_version:
    input:
        # This is a directory, no particular file is of importance
        'scan2/r-scan2_specific_commit'
    output:
        dir=directory('scan2/SCAN2_specific_commit'),
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts=directory("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=directory("scan2/SCAN2_specific_commit/resources")
    log:
    params:
        commit_id=config['scan2_version']['scan2']['installed_commit_id']
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        # rm: Snakemake creates empty directories to contain scan2 and Snakefile.
        # git-clone will not clone a repository into a non-empty directory.
        # 
        # bin/version.py - this file contains just a few variables that track the
        # version of the SCAN2 external pipeline (i.e., not the R package).  The
        # GIT_FULL_HASH inverse grep is to remove one templated line in the recipe
        # (templating causes the file to not be a valid YAML format).
        """
        rm -fr {output.dir}
        git clone https://github.com/parklab/SCAN2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        echo "version='$(cat recipes/scan2/meta.yaml | grep -v 'GIT_FULL_HASH' | shyaml get-value package.version)'" > bin/version.py
        echo "buildnum='$(cat recipes/scan2/meta.yaml | grep -v 'GIT_FULL_HASH' | shyaml get-value build.number)'" >> bin/version.py
        echo "githash='{params.commit_id}'" >> bin/version.py
        """


rule download_hs37d5:
    input:
    output:
        fasta='resources/human_g1k_v37_decoy.fasta',
        tmp_fasta=temp('resources/human_g1k_v37_decoy.fasta.gz'),
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        tmp_fai=temp('resources/human_g1k_v37_decoy.fasta.fai.gz'),
        fadict='resources/human_g1k_v37_decoy.dict',
        tmp_fadict=temp('resources/human_g1k_v37_decoy.dict.gz')
    shell:
        """
        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.gz \
            --output-document {output.tmp_fasta}
        gunzip -c {output.tmp_fasta} > {output.fasta}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.fai.gz \
            --output-document {output.tmp_fai}
        gunzip -c {output.tmp_fai} > {output.fai}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.dict.gz \
            --output-document {output.tmp_fadict}
        gunzip -c {output.tmp_fadict} > {output.fadict}
        """


rule download_shapeit_b37_refpanel:
    input:
    output:
        autosome_tgz=temp('resources/1000GP_Phase3.tgz'),
        chrX_tgz=temp('resources/1000GP_Phase3_chrX.tgz')
    threads: 1
    resources:
        mem_mb=250
    shell:
        """
        wget --output-document {output.autosome_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz
        wget --output-document {output.chrX_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3_chrX.tgz
        """


rule unpack_shapeit_b37_refpanel:
    input:
        autosome_tgz='resources/1000GP_Phase3.tgz',
        chrX_tgz='resources/1000GP_Phase3_chrX.tgz'
    output:
        dir=directory('resources/1000GP_Phase3')
    threads: 1
    resources:
        mem_mb=250
    shell:
        # The autosome tarball already has a top-level directory called 1000GP_Phase3,
        # so we only need --directory resources/. The chrX tarball does not have a top
        # level directory, so we have to supply the full path to --directory.
        """
        mkdir -p {output.dir}
        tar xzvf {input.autosome_tgz} --directory resources/
        tar xzvf {input.chrX_tgz} --directory {output.dir}
        """


rule scan2_cross_sample_panel_setup:
    input:
        # ancient(): allows for updates to SCAN2 without rerunning the panel, which
        # has *almost* no dependence on SCAN2.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts=ancient("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=ancient("scan2/SCAN2_specific_commit/resources"),
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        # It's ok to give scan2 makepanel a superset of the actual metadata.
        # I.e., we do not need to create a new metadata file with include.in.panel=FALSE 
        # rows filtered out.
        panel_metadata='metadata/immutable_metadata.csv',
        analyzable_regions='resources/scan2_panel_regions_chr1-22XY_12373windows_250kb.3kb_unanalyzable_region_removed.txt',
        # We do, however, have to not supply those BAMs.
        bams=config['panel_bams'],
        bais=[ bam.replace('.bam', '.bai') for bam in config['panel_bams'] ]
    output:
        yaml="scan2/panel/makepanel/scan.yaml"
    params:
        bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bams ])
    localrule: True
    threads: 1
    resources:
        mem_mb=4000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} config \
            --verbose \
            --analysis makepanel \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.analyzable_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --resources $(realpath {input.scan2_resources}) \
            --makepanel-metadata {input.panel_metadata} \
            {params.bam_flags}
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} validate
        """


rule scan2_cross_sample_panel:
    input:
        # ancient(): allows for updates to SCAN2 without rerunning the panel, which
        # has *almost* no dependence on SCAN2.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        yaml="scan2/panel/makepanel/scan.yaml"
    output:
        cross_sample_panel=protected("scan2/panel/makepanel/panel/panel.tab.gz"),
        benchmarks="scan2/panel/makepanel/makepanel_collected_benchmarks.txt"
    log:
        'scan2/panel/makepanel/log.txt'
    threads: 1
    resources:
        mem_mb=4000,
        scan2_queue=config['scan2_resources']['___INTERNAL___panel']['queue'],
        scan2_account=config['scan2_resources']['___INTERNAL___panel']['account'],
        scan2_runtime=config['scan2_resources']['___INTERNAL___panel']['runtime'],
        scan2_restart_times=config['scan2_resources']['___INTERNAL___panel']['restart_times'],
        scan2_joblimit=config['scan2_resources']['___INTERNAL___panel']['joblimit'],
        # These cores are only used in the final job that digests all of the GATK output
        scan2_panel_cores=10,
        # a little annoying: when no args are specified, want this to be an empty string.
        # when args are specified, add a space.
        scan2_other_snakemake_args='' if config['scan2_resources']['___INTERNAL___panel']['other_snakemake_args'] == '' else ' ' + config['scan2_resources']['___INTERNAL___panel']['other_snakemake_args'] 
    shell:
        # --notemp: the new scan2 rules that adds a dummy rule to create
        #   an argument file of VCFs (which are temped()) make it impossible
        #   to rerun the pipeline without recomputing everything.
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} \
            makepanel \
                --n-cores {resources.scan2_panel_cores} \
                --joblimit {resources.scan2_joblimit} \
                --snakemake-args ' --keep-going --restart-times={resources.scan2_restart_times} --max-status-checks-per-second 0.5 --max-jobs-per-second 1 --default-resources slurm_account={resources.scan2_account} slurm_partition={resources.scan2_queue} runtime={resources.scan2_runtime}{resources.scan2_other_snakemake_args}'
        """


rule scan2_call_mutations_setup:
    input:
        # ancient(): as above, allows the user to manually update SCAN2 without retriggering
        # a full SCAN2 run.  this is sometimes necessary when developing.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts=ancient("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=ancient("scan2/SCAN2_specific_commit/resources"),
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        analysis_regions='resources/scan2_analysis_regions_chr1-22XY_1252windows_2500kb.3kb_unanalyzable_region_removed.txt',
        sc_bams=lambda wildcards: bams[wildcards.donor]['single_cell'].values(),
        sc_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['single_cell'].values() ],
        bulk_bams=lambda wildcards: bams[wildcards.donor]['bulk'].values(),
        bulk_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['bulk'].values() ],
        cross_sample_panel="scan2/panel/makepanel/panel/panel.tab.gz"
    output:
        yaml="scan2/{donor}/scan2/scan.yaml"
    params:
        dir=directory("scan2/{donor}/scan2"),
        donor_sex=lambda wildcards: 'male' if config['donor_sex'][wildcards.donor] == 'M' else ('female' if config['donor_sex'][wildcards.donor] == 'F' else 'unknown'),
        sample_amp=lambda wildcards: ' '.join([ '--amplification ' + sample + ' ' + config['sample_amp'][sample] for sample in bams[wildcards.donor]['single_cell'].keys() ]), #config['donor_to_scan2_sample_map'][wildcards.donor] ]),
        sc_bam_flags=lambda wildcards, input: ' '.join([ '--sc-bam ' + bam for bam in input.sc_bams ]),
        bulk_bam_flag=lambda wildcards, input: '--bulk-bam ' + input.bulk_bams[0],
        other_bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bulk_bams[-0] ])
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --sex {params.donor_sex} \
            --analysis call_mutations \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.analysis_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --resources $(realpath {input.scan2_resources}) \
            {params.sc_bam_flags} \
            {params.bulk_bam_flag} \
            {params.sample_amp} \
            --cross-sample-panel {input.cross_sample_panel}
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_call_mutations:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        yaml="scan2/{donor}/scan2/scan.yaml"
    output:
        dpmatrix=protected("scan2/{donor}/scan2/depth_profile/joint_depth_matrix.tab.gz"),
        dpmatrixidx=protected("scan2/{donor}/scan2/depth_profile/joint_depth_matrix.tab.gz.tbi")
    params:
        scan2_output_dir="scan2/{donor}/scan2"
    log:
        'scan2/{donor}/scan2/log.txt'
    threads: 1
    resources:
        mem_mb=4000,
        scan2jobs=1,
        # Try encoding SCAN2 resources as resources in this rule rather than
        # parameters.  Changing a parameter will cause Snakemake to rerun jobs
        # but perhaps changing a resource won't.  This might not work, though,
        # because it will change the shell command, but is worth a try.
        scan2_queue=lambda wildcards: config['scan2_resources'][wildcards.donor]['queue'],
        scan2_account=lambda wildcards: config['scan2_resources'][wildcards.donor]['account'],
        scan2_runtime=lambda wildcards: config['scan2_resources'][wildcards.donor]['runtime'],
        scan2_abests_and_mutmodels_n_cores=lambda wildcards: config['scan2_resources'][wildcards.donor]['abests_and_mutmodels_n_cores'],
        scan2_ab_covariates_n_cores=lambda wildcards: config['scan2_resources'][wildcards.donor]['ab_covariates_n_cores'],
        scan2_restart_times=lambda wildcards: config['scan2_resources'][wildcards.donor]['restart_times'],
        scan2_joblimit=lambda wildcards: config['scan2_resources'][wildcards.donor]['joblimit'],
        # a little annoying: when no args are specified, want this to be an empty string.
        # when args are specified, add a space.
        scan2_other_snakemake_args=lambda wildcards: '' if config['scan2_resources'][wildcards.donor]['other_snakemake_args'] == '' else ' ' + config['scan2_resources'][wildcards.donor]['other_snakemake_args'] 
    shell:
        # --notemp: the new scan2 rules that adds a dummy rule to create
        #   an argument file of VCFs (which are temped()) make it impossible
        #   to rerun the pipeline without recomputing everything.
        """
        {input.scan2_bin} -d {params.scan2_output_dir} --snakefile {input.scan2_snakefile} \
            call_mutations \
                --joblimit {resources.scan2_joblimit} \
                --abests-and-mutmodels-n-cores={resources.scan2_abests_and_mutmodels_n_cores} \
                --ab-covariates-n-cores={resources.scan2_ab_covariates_n_cores} \
                --snakemake-args ' --restart-times={resources.scan2_restart_times} --keep-going --max-status-checks-per-second 0.1 --max-jobs-per-second 1 --default-resources slurm_account={resources.scan2_account} slurm_partition={resources.scan2_queue} runtime={resources.scan2_runtime}{resources.scan2_other_snakemake_args}'
        """


rule scan2_rda_to_vcf:
    input:
        rda=lambda wildcards: "scan2/" + config['sample_to_donor_map'][wildcards.sample] + "/scan2/sensitivity/{sample}/scan2_object.rda"
    output:
        vcf="vcfs/{qualtype}/{sample}.vcf"
    log:
        "vcfs/{qualtype}/{sample}.log"
    params:
        qualtype="{qualtype}"
    resources:
        # The full SCAN2 objects are now very large due to the spatial sensitivity models
        mem_mb=10000
    script:
        "scripts/scan2_to_vcf.R"


rule scan2_rescue_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        scan2_objects=lambda wildcards: config['scan2_rescue_groups'][wildcards.rescue_group].keys()
    output:
        "scan2/rescue_{rescue_group}/scan.yaml"
    log:
        "scan2/rescue_{rescue_group}/setup.log"
    benchmark:
        "scan2/rescue_{rescue_group}/setup.benchmark.txt"
    params:
        outdir="scan2/rescue_{rescue_group}",
        obj_flags=lambda wildcards, input: [ '--scan2-object ' + config['scan2_rescue_groups'][wildcards.rescue_group][obj] + ' ' + obj for obj in input.scan2_objects ]
    localrule: True
    threads: 1
    resources:
        # The (much) larger SCAN2 objects with spatial sensitivity models are ~9GB each.
        # An extra 3GB is needed for a very unfortunate required copy of the object@gatk
        # data.table.
        mem_mb=1000,
        localjob=1
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --analysis rescue \
            --rescue-target-fdr 0.01 \
            {params.obj_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_rescue_run:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        #scan2_objects=lambda wildcards: config['rescue_group_to_scan2_objects'][wildcards.rescue_group].keys(),
        scan2_objects=lambda wildcards: config['scan2_rescue_groups'][wildcards.rescue_group].keys(),
        yaml="scan2/rescue_{rescue_group}/scan.yaml"
    output:
        "scan2/rescue_{rescue_group}/rescued_muts.txt",
        "scan2/rescue_{rescue_group}/sig_homogeneity_tests.txt"
    log:
        "scan2/rescue_{rescue_group}/run.log"
    benchmark:
        "scan2/rescue_{rescue_group}/run.benchmark.txt"
    params:
        outdir="scan2/rescue_{rescue_group}",
    threads: 10
    resources:
        # The (much) larger SCAN2 objects with spatial sensitivity models are ~9GB each.
        # An extra 3GB is needed for a very unfortunate required copy of the object@gatk
        # data.table.  Just alotting double the object size here in case the entire object
        # is ever copied.
        mem_mb=lambda wildcards, input, threads: 1000 + (2*9000)*threads
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} rescue \
            --rescue-n-cores {threads} \
            --joblimit {threads}
        """


rule scan2_digest_calls:
    input:
        scan2_dir='scan2/SCAN2_specific_commit',
        muts="scan2/rescue_{rescue_group}/rescued_muts.txt",
        metadata='metadata/immutable_metadata.csv'
    output:
        unfiltered="scan2/{rescue_group}_mutations.UNFILTERED.txt",
        filtered="scan2/{rescue_group}_mutations.FILTERED.txt",
    log:
        "scan2/{rescue_group}_mutations.log"
    benchmark:
        "scan2/{rescue_group}_mutations.benchmark.txt"
    params:
        column_name='donor',
        filter_column='16'
    localrule: True
    threads: 1
    resources:
        mem_mb=500,
        localjob=1
    shell:
        """
        {input.scan2_dir}/bin/digest_calls.R \
            --muts {input.muts} \
            --metadata {input.metadata} \
            --individual-column {params.column_name} \
            {output.unfiltered} &> {log}

        awk -F, 'NR == 1 || ${params.filter_column} == "FALSE"' {output.unfiltered} > {output.filtered}
        """


rule scan2_table_to_csv:
    input:
        csv="scan2/{rescue_group}_mutations.{filter}.txt"
    output:
        csv="tables/{rescue_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{rescue_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=config['all_samples']  # Don't filter by sample. Also config['all_samples'] is a superset, so fairly meangingless
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


rule scan2_combine_all_tables:
    input:
        txts=expand('scan2/{rescue_group}_mutations.{{filter}}.txt',
            rescue_group=config['scan2_rescue_groups'].keys())
    output:
        csv="tables/all___{filter}_mut___any.csv"
    log:
        "tables/all___{filter}_mut___any.log"
    threads: 1
    localrule: True
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        (head -1 {input[0]} ; \
         tail --quiet -n +2 {input} \
            | sort -t, -k2 -k3 -n) \
            > {output.csv}
        """


rule make_synthetic_group_table:
    input:
        csv="tables/all___{filter}_mut___any.csv"
    output:
        csv="tables/{synthetic_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{synthetic_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=lambda wildcards: config['synthetic_groups'][wildcards.synthetic_group]
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


# Some tools, e.g. SnpEff, may need VCF input
rule scan2_table_to_vcf:
    input:
        csv="tables/{rescue_or_synthetic_group}___{filter}_mut___{qualtype}.csv"
    output:
        vcf="vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.vcf"
    log:
        "vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=config['all_samples']  # Don't filter by sample. Also config['all_samples'] is a superset, so fairly meangingless
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_vcf.R"


rule scan2_permtool_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        genome_file="resources/hg19.genome",
        muts="scan2/{rescue_group}_mutations.FILTERED.txt"
    output:
        "scan2/permtool/{rescue_group}/scan.yaml"
    log:
        "scan2/permtool/{rescue_group}/setup.log"
    benchmark:
        "scan2/permtool/{rescue_group}/setup.benchmark.txt"
    params:
        outdir="scan2/permtool/{rescue_group}",
        n_permutations=10000,
        # the SCAN2 directory is used by permtool for the joint depth matrix.
        # this is used to calculate the callable subset of the genome over which
        # to permute mutations.
        sample_flags=lambda wildcards: [ '--permtool-sample ' + sample + ' scan2/' + list(config['metadata'][config['metadata']['sample']==sample]['donor'])[0] + '/scan2' for sample in config['scan2_rescue_groups'][wildcards.rescue_group].values() ]
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --analysis permtool \
            --permtool-muts {input.muts} \
            --permtool-bedtools-genome-file {input.genome_file} \
            --permtool-n-permutations {params.n_permutations} \
            --permtool-callable-bed-n-cores 10 \
            --permtool-make-permutations-n-cores 10 \
            {params.sample_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_permtool_run:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        yaml="scan2/permtool/{rescue_group}/scan.yaml"
    output:
        expand("scan2/permtool/{{rescue_group}}/perms_{muttype}_{passtype}.rda",
            muttype=[ 'snv', 'indel'], passtype=[ 'pass', 'rescue' ])
    log:
        "scan2/permtool/{rescue_group}/run.log"
    benchmark:
        "scan2/permtool/{rescue_group}/run.benchmark.txt"
    params:
        outdir="scan2/permtool/{rescue_group}",
    threads: 1
    resources:
        mem_mb=4000
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} permtool \
            --joblimit 500 \
            --snakemake-args ' --keep-going --max-status-checks-per-second 0.5 --max-jobs-per-second 10 --slurm --default-resources slurm_account=park_contrib slurm_partition=park runtime=2880' \
        """


# To save compute resources, only run SCAN2 permtool on the exhaustive, mutually
# exclusive rescue groups. To create more permutation groups for, e.g., batch1 vs.
# batch2 analysis or bootstrapping/subsampling analyses, the per-sample output of
# previous permtool runs from the exhaustive, mutually exclusive runs are reused.
# These additional permutation groups are called "synthetic groups", and the
# definition in Snakefile:wildcard_constraints will define exactly which groups
# are recognized as synthetic.
#
# IMPORTANT: spending the CPU time to rerun permtools for additional permutation
# groups should provide the SAME OUTPUT as reusing the old permutations because
# the set of somatic mutation calls does not change and scan2 permtool also would
# use the same sequence of random seeds.
rule scan2_permtool_synthetic:
    input:
        combine_script="scan2/SCAN2_specific_commit/scripts/combine_permutations.R",
        rdas=lambda wildcards: 
            [ "scan2/permtool/" + config['scan2_rescue_groups_reverse'][sample] + "/perms_by_sample/" + sample + "/{muttype}_{passtype}.rda"
            for sample in config['synthetic_groups'][wildcards.synthetic_group] ]
    output:
        perms="scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.rda",
        seeds="scan2/permtool/{synthetic_group}/seedinfo_{muttype}_{passtype}.rda"
    log:
        "scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.log",
    benchmark:
        "scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.benchmark.txt",
    params:
        genome='hs37d5'
    threads: 1
    resources:
        mem_mb=48000
    shell:
        """
        {input.combine_script} \
            {params.genome} \
            {output.perms} \
            {output.seeds} \
            {threads} \
            {input.rdas} >& {log}
        """


rule scan2_link_full_object:
    input:
        scan2_full_obj=lambda wildcards: "scan2/rescue_" + config['scan2_rescue_groups_reverse'][wildcards.sample] + "/objects/{sample}_scan2_object_rescue.rda"
    output:
        linked_obj="scan2/full_objects/{sample}.rda"
    localrule: True
    threads: 1
    resources:
        mem_mb=10,
        localjob=1
    shell:
        """
        ln -s ../../{input.scan2_full_obj} {output.linked_obj}
        """


# Full SCAN2 objects are REALLY big--like ~10GB in size--and can therefore take an
# extremely long time to load. Loading 100+ of them can be totally impractical.
# Tiny objects have all summary data available, but truncated @gatk tables and
# @spatial.sensitivity models.
#
# Tiny objects are ~20-30 MB in size.
rule scan2_make_tiny_object:
    input:
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        scan2_obj="scan2/full_objects/{sample}.rda"
    output:
        rda="scan2/tiny_objects/{sample}.rda"
    log:
        "scan2/tiny_objects/{sample}.log"
    benchmark:
        "scan2/tiny_objects/{sample}.benchmark.txt"
    threads: 1
    resources:
        mem_mb=20000
    shell:
        """
        {input.scan2_scripts}/make_tiny_object.R {input.scan2_obj} {output.rda} &> {log}
        """
