# vim: syntax=python

bams = config['bams']
all_bams = config['all_bams']


# I don't know how to check the git commit ID of the installed R package (or
# how to condition a snakemake rule on a commit ID rather than an output file).
# So this rule would not be capable of keeping the R version updated, but will
# work once. It's mostly here for posterity.
rule download_specific_scan2_rpkg_version:
    input:
    output:
        # The real result of this rule is installing the R package. This directory
        # just serves as a flawed way of forcing this rule to run in roughly the
        # right dependency order.
        dir=directory('scan2/r-scan2_specific_commit')
    log:
    params:
        commit_id="aafa87ddb4151ab06f41e6479185d638023c5e31"
        # Real commit used is below. Contains several minor changes to the r-scan2
        # library that are necessary for scripts in this project, but do not affect
        # the SCAN2 mutation calling pipeline. Since the mutation caller is unaffected,
        # it wouldn't make sense to rerun all calling to use the latest commit.
        # commit_id="a3d90e715628d34b03862947f43981cc2e62cab"
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        git clone https://github.com/parklab/r-scan2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        R CMD INSTALL --build .
        """


rule download_specific_scan2_version:
    input:
        # This is a directory, no particular file is of importance
        'scan2/r-scan2_specific_commit'
    output:
        dir=directory('scan2/SCAN2_specific_commit'),
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts=directory("scan2/SCAN2_specific_commit/scripts")
    log:
    params:
        commit_id="b8858c1eb4cea9a7bc8d2d004f7a6c97ef473140"
        # Real commit used is below. This contains one update to a script that
        # summarizes tables at the very end of the SCAN2 pipeline, so it isn't
        # worth rerunning all of SCAN2 to update this param.
        #commit_id="79ec476ce86422ea502373c0ab7b6c5ea2e0ed46"
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        # rm: Snakemake creates empty directories to contain scan2 and Snakefile.
        # git-clone will not clone a repository into a non-empty directory.
        """
        rm -fr {output.dir}
        git clone https://github.com/parklab/SCAN2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        """


rule download_hs37d5:
    input:
    output:
        fasta='resources/human_g1k_v37_decoy.fasta',
        tmp_fasta=temp('resources/human_g1k_v37_decoy.fasta.gz'),
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        tmp_fai=temp('resources/human_g1k_v37_decoy.fasta.fai.gz'),
        fadict='resources/human_g1k_v37_decoy.dict',
        tmp_fadict=temp('resources/human_g1k_v37_decoy.dict.gz')
    shell:
        """
        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.gz \
            --output-document {output.tmp_fasta}
        gunzip -c {output.tmp_fasta} > {output.fasta}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.fai.gz \
            --output-document {output.tmp_fai}
        gunzip -c {output.tmp_fai} > {output.fai}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.dict.gz \
            --output-document {output.tmp_fadict}
        gunzip -c {output.tmp_fadict} > {output.fadict}
        """


rule download_shapeit_b37_refpanel:
    input:
    output:
        autosome_tgz=temp('resources/1000GP_Phase3.tgz'),
        chrX_tgz=temp('resources/1000GP_Phase3_chrX.tgz')
    threads: 1
    resources:
        mem_mb=250
    shell:
        """
        wget --output-document {output.autosome_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz
        wget --output-document {output.chrX_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3_chrX.tgz
        """


rule unpack_shapeit_b37_refpanel:
    input:
        autosome_tgz='resources/1000GP_Phase3.tgz',
        chrX_tgz='resources/1000GP_Phase3_chrX.tgz'
    output:
        dir=directory('resources/1000GP_Phase3')
    threads: 1
    resources:
        mem_mb=250
    shell:
        # The autosome tarball already has a top-level directory called 1000GP_Phase3,
        # so we only need --directory resources/. The chrX tarball does not have a top
        # level directory, so we have to supply the full path to --directory.
        """
        mkdir -p {output.dir}
        tar xzvf {input.autosome_tgz} --directory resources/
        tar xzvf {input.chrX_tgz} --directory {output.dir}
        """


rule scan2_cross_sample_panel_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        panel_metadata='metadata/panel_metadata_162cells_66ptaols_40mdaglia_56ptaneurons_21bulks_20brains.csv',
        gatk_regions='metadata/scan2_panel_regions_11537_windows_250kb.no_bad_chunk.txt',
        bams=all_bams,
        bais=[ bam.replace('.bam', '.bai') for bam in all_bams ]
    output:
        yaml="scan2/panel/makepanel/scan.yaml"
    params:
        bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bams ])
    localrule: True
    threads: 1
    resources:
        mem_mb=4000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} config \
            --verbose \
            --analysis makepanel \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.gatk_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --makepanel-metadata {input.panel_metadata} \
            {params.bam_flags}
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} validate
        """


rule scan2_cross_sample_panel:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        yaml="scan2/panel/makepanel/scan.yaml"
    output:
        cross_sample_panel=protected("scan2/panel/makepanel/panel/panel.tab.gz"),
        benchmarks="scan2/panel/makepanel/makepanel_collected_benchmarks.txt"
    log:
        'scan2/panel/makepanel/log.txt'
    threads: 1
    resources:
        mem_mb=4000
    shell:
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} \
            makepanel \
                --joblimit 1000 \
                --drmaa ' -p park -A park_contrib -c {{threads}} --mem={{resources.mem_mb}} -t 40:00:00 -o %logdir/slurm-%A.log' \
                --snakemake-args ' --restart-times=2 --keep-going --max-status-checks-per-second 0.5 --max-jobs-per-second 2'
        """


rule scan2_call_mutations_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        gatk_regions='metadata/scan2_gatk_regions_1167windows_2500000_no_bad_chunk.txt',
        sc_bams=lambda wildcards: bams[wildcards.donor]['single_cell'].values(),
        sc_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['single_cell'].values() ],
        bulk_bams=lambda wildcards: bams[wildcards.donor]['bulk'].values(),
        bulk_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['bulk'].values() ],
        cross_sample_panel="scan2/panel/makepanel/panel/panel.tab.gz"
    output:
        yaml="scan2/{donor}/scan2/scan.yaml"
    params:
        dir=directory("scan2/{donor}/scan2"),
        sc_bam_flags=lambda wildcards, input: ' '.join([ '--sc-bam ' + bam for bam in input.sc_bams ]),
        bulk_bam_flag=lambda wildcards, input: '--bulk-bam ' + input.bulk_bams[0],
        other_bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bulk_bams[-0] ])
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --analysis call_mutations \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.gatk_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --abmodel-n-cores 10 \
            --sensitivity-n-cores 10 \
            {params.sc_bam_flags} \
            {params.bulk_bam_flag} \
            --cross-sample-panel {input.cross_sample_panel}
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_call_mutations:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        yaml="scan2/{donor}/scan2/scan.yaml"
    output:
        dpmatrix=protected("scan2/{donor}/scan2/depth_profile/joint_depth_matrix.tab.gz")
    params:
        scan2_output_dir="scan2/{donor}/scan2",
    log:
        'scan2/{donor}/scan2/log.txt'
    threads: 1
    resources:
        mem_mb=4000
    shell:
        # Very intentionally not putting --drmaa or --snakemake-args as Snakemake params.
        # Changing these do not change results, but Snakemake may want to rerun jobs
        # if the params ever did change.
        """
        {input.scan2_bin} -d {params.scan2_output_dir} --snakefile {input.scan2_snakefile} \
            run \
                --joblimit 800 \
                --drmaa ' -p park -A park_contrib -c {{threads}} --mem={{resources.mem_mb}} -t 24:00:00 -o %logdir/slurm-%A.log' \
                --snakemake-args ' --restart-times=2 --keep-going --max-status-checks-per-second 0.5 --max-jobs-per-second 2'
        """


rule scan2_rda_to_vcf:
    input:
        rda=lambda wildcards: "scan2/" + config['sample_to_donor_map'][wildcards.sample] + "/scan2/sensitivity/{sample}/scan2_object.rda"
    output:
        vcf="vcfs/{qualtype}/{sample}.vcf"
    log:
        "vcfs/{qualtype}/{sample}.log"
    params:
        qualtype="{qualtype}"
    resources:
        # The full SCAN2 objects are now very large due to the spatial sensitivity models
        mem_mb=10000
    script:
        "scripts/scan2_to_vcf.R"


rule scan2_rescue_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        scan2_objects=lambda wildcards: config['scan2_rescue_groups'][wildcards.rescue_group].keys()
    output:
        "scan2/rescue_{rescue_group}/scan.yaml"
    log:
        "scan2/rescue_{rescue_group}/setup.log"
    benchmark:
        "scan2/rescue_{rescue_group}/setup.benchmark.txt"
    params:
        outdir="scan2/rescue_{rescue_group}",
        obj_flags=lambda wildcards, input: [ '--scan2-object ' + config['scan2_rescue_groups'][wildcards.rescue_group][obj] + ' ' + obj for obj in input.scan2_objects ]
    localrule: True
    threads: 1
    resources:
        # The (much) larger SCAN2 objects with spatial sensitivity models are ~9GB each.
        # An extra 3GB is needed for a very unfortunate required copy of the object@gatk
        # data.table.
        mem_mb=1000,
        localjob=1
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --analysis rescue \
            --rescue-target-fdr 0.01 \
            {params.obj_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_rescue_run:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        scan2_objects=lambda wildcards: rescue_group_to_scan2_objects(wildcards.rescue_group).keys(),
        yaml="scan2/rescue_{rescue_group}/scan.yaml"
    output:
        "scan2/rescue_{rescue_group}/rescued_muts.txt",
        "scan2/rescue_{rescue_group}/sig_homogeneity_tests.txt"
    log:
        "scan2/rescue_{rescue_group}/run.log"
    benchmark:
        "scan2/rescue_{rescue_group}/run.benchmark.txt"
    params:
        outdir="scan2/rescue_{rescue_group}",
    threads: 10
    resources:
        # The (much) larger SCAN2 objects with spatial sensitivity models are ~9GB each.
        # An extra 3GB is needed for a very unfortunate required copy of the object@gatk
        # data.table.  Just alotting double the object size here in case the entire object
        # is ever copied.
        mem_mb=lambda wildcards, input, threads: 1000 + (2*9000)*threads
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} rescue \
            --rescue-n-cores {threads} \
            --joblimit {threads}
        """


rule scan2_digest_calls:
    input:
        scan2_dir='scan2/SCAN2_specific_commit',
        muts="scan2/rescue_{rescue_group}/rescued_muts.txt",
        metadata='metadata/sample_metadata.csv'
    output:
        unfiltered="scan2/{rescue_group}_mutations.UNFILTERED.txt",
        filtered="scan2/{rescue_group}_mutations.FILTERED.txt",
    log:
        "scan2/{rescue_group}_mutations.log"
    benchmark:
        "scan2/{rescue_group}_mutations.benchmark.txt"
    params:
        column_name='donor',
        filter_column='16'
    localrule: True
    threads: 1
    resources:
        mem_mb=500,
        localjob=1
    shell:
        """
        {input.scan2_dir}/bin/digest_calls.R \
            --muts {input.muts} \
            --metadata {input.metadata} \
            --individual-column {params.column_name} \
            {output.unfiltered} &> {log}

        awk -F, 'NR == 1 || ${params.filter_column} == "FALSE"' {output.unfiltered} > {output.filtered}
        """


rule scan2_table_to_csv:
    input:
        csv="scan2/{rescue_group}_mutations.{filter}.txt"
    output:
        csv="tables/{rescue_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{rescue_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=config['all_samples']  # Don't filter by sample
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


rule scan2_combine_all_tables:
    input:
        txts=expand('scan2/{rescue_group}_mutations.{{filter}}.txt',
            rescue_group=config['scan2_rescue_groups'].keys())
    output:
        csv="tables/all___{filter}_mut___any.csv"
    log:
        "tables/all___{filter}_mut___any.log"
    threads: 1
    localrule: True
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        (head -1 {input[0]} ; \
         tail --quiet -n +2 {input} \
            | sort -t, -k2 -k3 -n) \
            > {output.csv}
        """


rule make_synthetic_group_table:
    input:
        csv="tables/all___{filter}_mut___any.csv"
    output:
        csv="tables/{synthetic_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{synthetic_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=lambda wildcards: config['synthetic_groups'][wildcards.synthetic_group]
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


# Some tools, e.g. SnpEff, may need VCF input
rule scan2_table_to_vcf:
    input:
        csv="tables/{rescue_or_synthetic_group}___{filter}_mut___{qualtype}.csv"
    output:
        vcf="vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.vcf"
    log:
        "vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=config['all_samples']  # Don't filter by sample
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_vcf.R"


rule scan2_permtool_setup:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        genome_file="resources/hg19.genome",
        muts="scan2/{rescue_group}_mutations.FILTERED.txt"
    output:
        "scan2/permtool/{rescue_group}/scan.yaml"
    log:
        "scan2/permtool/{rescue_group}/setup.log"
    benchmark:
        "scan2/permtool/{rescue_group}/setup.benchmark.txt"
    params:
        outdir="scan2/permtool/{rescue_group}",
        n_permutations=10000,
        # the SCAN2 directory is used by permtool for the joint depth matrix.
        # this is used to calculate the callable subset of the genome over which
        # to permute mutations.
        sample_flags=lambda wildcards: [ '--permtool-sample ' + sample + ' scan2/' + list(config['metadata'][config['metadata']['sample']==sample]['donor'])[0] + '/scan2' for sample in config['scan2_rescue_groups'][wildcards.rescue_group].values() ]
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --analysis permtool \
            --permtool-muts {input.muts} \
            --permtool-bedtools-genome-file {input.genome_file} \
            --permtool-n-permutations {params.n_permutations} \
            --permtool-callable-bed-n-cores 10 \
            --permtool-make-permutations-n-cores 10 \
            {params.sample_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_permtool_run:
    input:
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        yaml="scan2/permtool/{rescue_group}/scan.yaml"
    output:
        expand("scan2/permtool/{{rescue_group}}/perms_{muttype}_{passtype}.rda",
            muttype=[ 'snv', 'indel'], passtype=[ 'pass', 'rescue' ])
    log:
        "scan2/permtool/{rescue_group}/run.log"
    benchmark:
        "scan2/permtool/{rescue_group}/run.benchmark.txt"
    params:
        outdir="scan2/permtool/{rescue_group}",
    threads: 1
    resources:
        mem_mb=4000
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} permtool \
            --joblimit 500 \
            --drmaa ' -p park -A park_contrib -c {{threads}} --mem={{resources.mem_mb}} -t 40:00:00 -o %logdir/slurm-%A.log' \
            --snakemake-args ' --keep-going --max-status-checks-per-second 0.5 --max-jobs-per-second 2'
        """


# To save compute resources, only run SCAN2 permtool on the exhaustive, mutually
# exclusive rescue groups. To create more permutation groups for, e.g., batch1 vs.
# batch2 analysis or bootstrapping/subsampling analyses, the per-sample output of
# previous permtool runs from the exhaustive, mutually exclusive runs are reused.
# These additional permutation groups are called "synthetic groups", and the
# definition in Snakefile:wildcard_constraints will define exactly which groups
# are recognized as synthetic.
#
# IMPORTANT: spending the CPU time to rerun permtools for additional permutation
# groups should provide the SAME OUTPUT as reusing the old permutations because
# the set of somatic mutation calls does not change and scan2 permtool also would
# use the same sequence of random seeds.
rule scan2_permtool_synthetic:
    input:
        combine_script="scan2/SCAN2_specific_commit/scripts/combine_permutations.R",
        rdas=lambda wildcards: 
            [ "scan2/permtool/" + config['scan2_rescue_groups_reverse'][sample] + "/perms_by_sample/" + sample + "/{muttype}_{passtype}.rda"
            for sample in config['synthetic_groups'][wildcards.synthetic_group] ]
    output:
        perms="scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.rda",
        seeds="scan2/permtool/{synthetic_group}/seedinfo_{muttype}_{passtype}.rda"
    log:
        "scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.log",
    benchmark:
        "scan2/permtool/{synthetic_group}/perms_{muttype}_{passtype}.benchmark.txt",
    params:
        genome='hs37d5'
    threads: 1
    resources:
        mem_mb=48000
    shell:
        """
        {input.combine_script} \
            {params.genome} \
            {output.perms} \
            {output.seeds} \
            {threads} \
            {input.rdas} >& {log}
        """


rule scan2_link_full_object:
    input:
        scan2_full_obj=lambda wildcards: "scan2/rescue_" + config['scan2_rescue_groups_reverse'][wildcards.sample] + "/objects/{sample}_scan2_object_rescue.rda"
    output:
        linked_obj="scan2/full_objects/{sample}.rda"
    localrule: True
    threads: 1
    resources:
        mem_mb=10,
        localjob=1
    shell:
        """
        ln -s ../../{input.scan2_full_obj} {output.linked_obj}
        """


# Full SCAN2 objects are REALLY big--like ~10GB in size--and can therefore take an
# extremely long time to load. Loading 100+ of them can be totally impractical.
# Tiny objects have all summary data available, but truncated @gatk tables and
# @spatial.sensitivity models.
#
# Tiny objects are ~20-30 MB in size.
rule scan2_make_tiny_object:
    input:
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        scan2_obj="scan2/full_objects/{sample}.rda"
    output:
        rda="scan2/tiny_objects/{sample}.rda"
    log:
        "scan2/tiny_objects/{sample}.log"
    benchmark:
        "scan2/tiny_objects/{sample}.benchmark.txt"
    threads: 1
    resources:
        mem_mb=20000
    shell:
        """
        {input.scan2_scripts}/make_tiny_object.R {input.scan2_obj} {output.rda} &> {log}
        """
