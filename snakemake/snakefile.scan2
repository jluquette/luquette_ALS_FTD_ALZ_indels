# vim: syntax=python

bams = config['bams']
all_bams = config['all_bams']


# I don't know how to check the git commit ID of the installed R package (or
# how to condition a snakemake rule on a commit ID rather than an output file).
# So this rule would not be capable of keeping the R version updated, but will
# work once. It's mostly here for posterity.
rule download_specific_scan2_rpkg_version:
    input:
    output:
        # The real result of this rule is installing the R package. This directory
        # just serves as a flawed way of forcing this rule to run in roughly the
        # right dependency order.
        dir=directory('scan2/r-scan2_specific_commit')
    log:
    params:
        commit_id=config['scan2_version']['r-scan2']['installed_commit_id']
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        # The git clone does not actually provide the r-scan2 package.  It merely
        # imports the source at the correct commit ID as a reference.  The install_github
        # call actually installs the R package directly from github.  The reason we use
        # devtools' install_github() is to inject the commit ID into the installed R package's
        # version data, enabling version tracking.
        """
        rm -fr {output.dir}
        git clone https://github.com/parklab/r-scan2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        Rscript -e 'devtools::install_github("parklab/r-scan2@{params.commit_id}", upgrade="never")'
        """


rule download_specific_scan2_version:
    input:
        # This is a directory, no particular file is of importance
        'scan2/r-scan2_specific_commit'
    output:
        dir=directory('scan2/SCAN2_specific_commit'),
        scan2_bin="scan2/SCAN2_specific_commit/bin/scan2",
        scan2_snakefile="scan2/SCAN2_specific_commit/snakemake/Snakefile",
        scan2_scripts=directory("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=directory("scan2/SCAN2_specific_commit/resources")
    log:
    params:
        commit_id=config['scan2_version']['scan2']['installed_commit_id']
    localrule: True
    threads: 1
    resources:
        mem_mb=250,
        localjob=1
    shell:
        # rm: Snakemake creates empty directories to contain scan2 and Snakefile.
        # git-clone will not clone a repository into a non-empty directory.
        # 
        # bin/version.py - this file contains just a few variables that track the
        # version of the SCAN2 external pipeline (i.e., not the R package).  The
        # GIT_FULL_HASH inverse grep is to remove one templated line in the recipe
        # (templating causes the file to not be a valid YAML format).
        """
        rm -fr {output.dir}
        git clone https://github.com/parklab/SCAN2.git {output.dir}
        cd {output.dir}
        git checkout -q {params.commit_id}
        echo "version='$(cat recipes/scan2/meta.yaml | grep -v 'GIT_FULL_HASH' | shyaml get-value package.version)'" > bin/version.py
        echo "buildnum='$(cat recipes/scan2/meta.yaml | grep -v 'GIT_FULL_HASH' | shyaml get-value build.number)'" >> bin/version.py
        echo "githash='{params.commit_id}'" >> bin/version.py
        """


rule download_hs37d5:
    input:
    output:
        fasta='resources/human_g1k_v37_decoy.fasta',
        tmp_fasta=temp('resources/human_g1k_v37_decoy.fasta.gz'),
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        tmp_fai=temp('resources/human_g1k_v37_decoy.fasta.fai.gz'),
        fadict='resources/human_g1k_v37_decoy.dict',
        tmp_fadict=temp('resources/human_g1k_v37_decoy.dict.gz')
    shell:
        """
        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.gz \
            --output-document {output.tmp_fasta}
        gunzip -c {output.tmp_fasta} > {output.fasta}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.fasta.fai.gz \
            --output-document {output.tmp_fai}
        gunzip -c {output.tmp_fai} > {output.fai}

        wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37_decoy.dict.gz \
            --output-document {output.tmp_fadict}
        gunzip -c {output.tmp_fadict} > {output.fadict}
        """


rule download_shapeit_b37_refpanel:
    input:
    output:
        autosome_tgz=temp('resources/1000GP_Phase3.tgz'),
        chrX_tgz=temp('resources/1000GP_Phase3_chrX.tgz')
    threads: 1
    resources:
        mem_mb=250
    shell:
        """
        wget --output-document {output.autosome_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz
        wget --output-document {output.chrX_tgz} \
            https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3_chrX.tgz
        """


rule unpack_shapeit_b37_refpanel:
    input:
        autosome_tgz='resources/1000GP_Phase3.tgz',
        chrX_tgz='resources/1000GP_Phase3_chrX.tgz'
    output:
        dir=directory('resources/1000GP_Phase3')
    threads: 1
    resources:
        mem_mb=250
    shell:
        # The autosome tarball already has a top-level directory called 1000GP_Phase3,
        # so we only need --directory resources/. The chrX tarball does not have a top
        # level directory, so we have to supply the full path to --directory.
        """
        mkdir -p {output.dir}
        tar xzvf {input.autosome_tgz} --directory resources/
        tar xzvf {input.chrX_tgz} --directory {output.dir}
        """


rule scan2_cross_sample_panel_setup:
    input:
        # ancient(): allows for updates to SCAN2 without rerunning the panel, which
        # has *almost* no dependence on SCAN2.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts=ancient("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=ancient("scan2/SCAN2_specific_commit/resources"),
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        # It's ok to give scan2 makepanel a superset of the actual metadata.
        # I.e., we do not need to create a new metadata file with include.in.panel=FALSE 
        # rows filtered out.
        panel_metadata='metadata/immutable_metadata.csv',
        analyzable_regions='resources/scan2_panel_regions_chr1-22XY_12373windows_250kb.3kb_unanalyzable_region_removed.txt',
        # We do, however, have to not supply those BAMs.
        bams=config['panel_bams'],
        bais=[ bam.replace('.bam', '.bai') for bam in config['panel_bams'] ]
    output:
        yaml="scan2/panel/makepanel/scan.yaml"
    params:
        bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bams ])
    localrule: True
    threads: 1
    resources:
        mem_mb=4000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} config \
            --verbose \
            --analysis makepanel \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.analyzable_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --resources $(realpath {input.scan2_resources}) \
            --makepanel-metadata {input.panel_metadata} \
            {params.bam_flags}
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} validate
        """


rule scan2_cross_sample_panel:
    input:
        # ancient(): allows for updates to SCAN2 without rerunning the panel, which
        # has *almost* no dependence on SCAN2.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        yaml="scan2/panel/makepanel/scan.yaml"
    output:
        cross_sample_panel=protected("scan2/panel/makepanel/panel/panel.tab.gz"),
        benchmarks="scan2/panel/makepanel/makepanel_collected_benchmarks.txt"
    log:
        'scan2/panel/makepanel/log.txt'
    threads: 1
    resources:
        mem_mb=4000,
        scan2_queue=config['scan2_resources']['___INTERNAL___panel']['queue'],
        scan2_account=config['scan2_resources']['___INTERNAL___panel']['account'],
        scan2_runtime=config['scan2_resources']['___INTERNAL___panel']['runtime'],
        scan2_restart_times=config['scan2_resources']['___INTERNAL___panel']['restart_times'],
        scan2_joblimit=config['scan2_resources']['___INTERNAL___panel']['joblimit'],
        # These cores are only used in the final job that digests all of the GATK output
        scan2_panel_cores=10,
        # a little annoying: when no args are specified, want this to be an empty string.
        # when args are specified, add a space.
        scan2_other_snakemake_args='' if config['scan2_resources']['___INTERNAL___panel']['other_snakemake_args'] == '' else ' ' + config['scan2_resources']['___INTERNAL___panel']['other_snakemake_args'] 
    shell:
        # --notemp: the new scan2 rules that adds a dummy rule to create
        #   an argument file of VCFs (which are temped()) make it impossible
        #   to rerun the pipeline without recomputing everything.
        """
        {input.scan2_bin} -d scan2/panel/makepanel --snakefile {input.scan2_snakefile} \
            makepanel \
                --n-cores {resources.scan2_panel_cores} \
                --joblimit {resources.scan2_joblimit} \
                --snakemake-args ' --keep-going --restart-times={resources.scan2_restart_times} --max-status-checks-per-second 0.5 --max-jobs-per-second 1 --default-resources slurm_account={resources.scan2_account} slurm_partition={resources.scan2_queue} runtime={resources.scan2_runtime}{resources.scan2_other_snakemake_args}'
        """


rule scan2_call_mutations_setup:
    input:
        # ancient(): as above, allows the user to manually update SCAN2 without retriggering
        # a full SCAN2 run.  this is sometimes necessary when developing.
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts=ancient("scan2/SCAN2_specific_commit/scripts"),
        scan2_resources=ancient("scan2/SCAN2_specific_commit/resources"),
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        shapeit_panel='resources/1000GP_Phase3',
        dbsnp='resources/dbsnp_147_b37_common_all_20160601.vcf',
        analysis_regions='resources/scan2_analysis_regions_chr1-22XY_1252windows_2500kb.3kb_unanalyzable_region_removed.txt',
        sc_bams=lambda wildcards: bams[wildcards.donor]['single_cell'].values(),
        sc_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['single_cell'].values() ],
        bulk_bams=lambda wildcards: bams[wildcards.donor]['bulk'].values(),
        bulk_bais=lambda wildcards: [ bam.replace('.bam', '.bai') for bam in bams[wildcards.donor]['bulk'].values() ],
        cross_sample_panel="scan2/panel/makepanel/panel/panel.tab.gz"
    output:
        yaml="scan2/{donor}/scan2/scan.yaml"
    params:
        dir=directory("scan2/{donor}/scan2"),
        donor_sex=lambda wildcards: 'male' if config['donor_sex'][wildcards.donor] == 'M' else ('female' if config['donor_sex'][wildcards.donor] == 'F' else 'unknown'),
        sample_amp=lambda wildcards: ' '.join([ '--amplification ' + sample + ' ' + config['sample_amp'][sample] for sample in bams[wildcards.donor]['single_cell'].keys() ]), #config['donor_to_scan2_sample_map'][wildcards.donor] ]),
        sc_bam_flags=lambda wildcards, input: ' '.join([ '--sc-bam ' + bam for bam in input.sc_bams ]),
        bulk_bam_flag=lambda wildcards, input: '--bulk-bam ' + input.bulk_bams[0],
        other_bam_flags=lambda wildcards, input: ' '.join([ '--bam ' + bam for bam in input.bulk_bams[-0] ])
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        # realpath: SCAN2 does not realpath directories by default. Need to supply the
        # full path because SCAN2 does change the working directory.
        """
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --sex {params.donor_sex} \
            --analysis call_mutations \
            --gatk sentieon_joint \
            --ref {input.fasta} \
            --dbsnp {input.dbsnp} \
            --shapeit-refpanel $(realpath {input.shapeit_panel}) \
            --regions-file {input.analysis_regions} \
            --scripts $(realpath {input.scan2_scripts}) \
            --resources $(realpath {input.scan2_resources}) \
            {params.sc_bam_flags} \
            {params.bulk_bam_flag} \
            {params.sample_amp} \
            --cross-sample-panel {input.cross_sample_panel}
        {input.scan2_bin} -d {params.dir} --snakefile {input.scan2_snakefile} validate
        """


# These jobs cannot be handled by a single rule because the output depends
# on the value of the wildcard 'donor', which is not allowed in snakemake.
# The reason is that this rule creates one output per sample associated with
# 'donor', which can only be known by accessing the config[] dict storing
# which samples are related to which donors. The snakemake way of handling
# this is checkpoints, but I've found checkpoints to be hard to work with
# because they make it impossible to compute the DAG up front.
for donor in config['scan2_donors']:
    rule:
        name: "scan2_call_mutations_" + donor
        input:
            scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
            scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
            yaml="scan2/"+ donor + "/scan2/scan.yaml"
        output:
            dpmatrix=protected("scan2/" + donor + "/scan2/depth_profile/joint_depth_matrix.tab.gz"),
            dpmatrixidx=protected("scan2/" + donor + "/scan2/depth_profile/joint_depth_matrix.tab.gz.tbi"),
            rdas=protected(expand('scan2/' + donor + '/scan2/call_mutations/{sample}/scan2_object.rda',
                sample=bams[donor]['single_cell'].keys()))
        params:
            scan2_output_dir="scan2/" + donor + "/scan2"
        log:
            "scan2/" + donor + "/scan2/log.txt"
        threads: 1
        resources:
            mem_mb=4000,
            scan2jobs=1,
            # Try encoding SCAN2 resources as resources in this rule rather than
            # parameters.  Changing a parameter will cause Snakemake to rerun jobs
            # but perhaps changing a resource won't.  This might not work, though,
            # because it will change the shell command, but is worth a try.
            scan2_queue=config['scan2_resources'][donor]['queue'],
            scan2_account=config['scan2_resources'][donor]['account'],
            scan2_runtime=config['scan2_resources'][donor]['runtime'],
            scan2_genotype_n_cores=1,
            scan2_abests_and_mutmodels_n_cores=config['scan2_resources'][donor]['abests_and_mutmodels_n_cores'],
            scan2_ab_covariates_n_cores=config['scan2_resources'][donor]['ab_covariates_n_cores'],
            scan2_restart_times=config['scan2_resources'][donor]['restart_times'],
            scan2_joblimit=config['scan2_resources'][donor]['joblimit'],
            # a little annoying: when no args are specified, want this to be an empty string.
            # when args are specified, add a space.
            scan2_other_snakemake_args= '' if config['scan2_resources'][donor]['other_snakemake_args'] == '' else ' ' + config['scan2_resources'][donor]['other_snakemake_args'] 
        shell:
            # --notemp: the new scan2 rules that adds a dummy rule to create
            #   an argument file of VCFs (which are temped()) make it impossible
            #   to rerun the pipeline without recomputing everything.
            """
            {input.scan2_bin} -d {params.scan2_output_dir} --snakefile {input.scan2_snakefile} \
                call_mutations \
                    --joblimit {resources.scan2_joblimit} \
                    --genotype-n-cores={resources.scan2_genotype_n_cores} \
                    --abests-and-mutmodels-n-cores={resources.scan2_abests_and_mutmodels_n_cores} \
                    --ab-covariates-n-cores={resources.scan2_ab_covariates_n_cores} \
                    --snakemake-args ' --restart-times={resources.scan2_restart_times} --keep-going --max-status-checks-per-second 0.1 --max-jobs-per-second 1 --default-resources slurm_account={resources.scan2_account} slurm_partition={resources.scan2_queue} runtime={resources.scan2_runtime}{resources.scan2_other_snakemake_args}'
            """


rule scan2_rescue_setup:
    input:
        fasta='resources/human_g1k_v37_decoy.fasta',
        fai='resources/human_g1k_v37_decoy.fasta.fai',
        fadict='resources/human_g1k_v37_decoy.dict',
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts=ancient("scan2/SCAN2_specific_commit/scripts"),
        scan2_objects=lambda wildcards: config['scan2_rescue_groups'][wildcards.rescue_group].keys()
    output:
        "scan2/rescue_{rescue_group}/scan.yaml"
    log:
        "scan2/rescue_{rescue_group}/setup.log"
    benchmark:
        "scan2/rescue_{rescue_group}/setup.benchmark.txt"
    params:
        outdir="scan2/rescue_{rescue_group}",
        obj_flags=lambda wildcards, input: [ '--scan2-object ' + config['scan2_rescue_groups'][wildcards.rescue_group][obj] + ' ' + obj for obj in input.scan2_objects ]
    localrule: True
    threads: 1
    resources:
        mem_mb=1000,
        localjob=1
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --analysis rescue \
            --ref {input.fasta} \
            --rescue-target-fdr 0.01 \
            {params.obj_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


# Same as scan2_call_mutations: outputs depend on wildcard 'group'
for group in config['scan2_rescue_groups'].keys():
    rule:
        name: 'scan2_rescue_run_' + group
        input:
            scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
            scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
            scan2_scripts="scan2/SCAN2_specific_commit/scripts",
            scan2_objects=config['scan2_rescue_groups'][group].keys(),
            yaml="scan2/rescue_" + group + "/scan.yaml"
        output:
            "scan2/rescue_" + group + "/rescued_muts.txt",
            "scan2/rescue_" + group + "/sig_homogeneity_tests.txt",
            rdas=expand('scan2/rescue_' + group + '/objects/{sample}_scan2_object_rescue.rda',
                sample=config['scan2_rescue_groups'][group].values())
        log:
            "scan2/rescue_" + group + "/run.log"
        benchmark:
            "scan2/rescue_" + group + "/run.benchmark.txt"
        params:
            outdir="scan2/rescue_" + group + ""
        threads: 10
        resources:
            # Each thread loads one full analysis object (~3.5Gb of RAM for humans).
            # Give double in case a copy is made.
            mem_mb=lambda wildcards, input, threads: (7000)*(1 + threads)
        shell:
            """
            {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} rescue \
                --n-cores {threads} \
                --joblimit {threads}
            """


# Make a flattened directory of SCAN2 objects for convenience. Does require
# that sample names are unique across donors.
rule scan2_link_full_object:
    input:
        # Pre-rescue object, if we ever want to go back to that
        #scan2_full_obj=lambda wildcards: "scan2/" + config['sample_to_donor_map'][wildcards.sample] + "/call_mutations/{sample}/scan2_object.rda"
        scan2_full_obj=lambda wildcards: "scan2/rescue_" + config['scan2_rescue_groups_reverse'][wildcards.sample] + "/objects/" + wildcards.sample + "_scan2_object_rescue.rda"
    output:
        linked_obj="scan2/full_objects/{sample}.rda"
    localrule: True
    threads: 1
    resources:
        mem_mb=10,
        localjob=1
    shell:
        """
        ln -s ../../{input.scan2_full_obj} {output.linked_obj}
        """


# Full objects are ~3 Gb for human samples, summaries are ~45 Mb and contain
# most of the information necessary for downstream analysis.
rule scan2_make_summary:
    input:
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        rda="scan2/full_objects/{sample}.rda"
    output:
        rda="scan2/summary_objects/{sample}.rda"
    log:
        "scan2/summary_objects/{sample}.log"
    benchmark:
        "scan2/summary_objects/{sample}.benchmark.txt"
    threads: 4
    resources:
        mem_mb=lambda wildcards, input, threads: (1+threads)*8000
    shell:
        """
        {input.scan2_scripts}/summarize_object.R {input.rda} {output.rda} {threads} &> {log}
        """


print("WARNING: scan2_digest_calls removes all chrX/chrY calls in mutations.FILTERED.txt to prevent using them in enrichment analyses. Ensure your sex chromosomes are named 'X' and 'Y'!")
rule scan2_digest_calls:
    input:
        scan2_dir='scan2/SCAN2_specific_commit',
        muts="scan2/rescue_{rescue_group}/rescued_muts.txt",
        metadata='metadata/immutable_metadata.csv'
    output:
        unfiltered="scan2/{rescue_group}_mutations.UNFILTERED.txt",
        filtered="scan2/{rescue_group}_mutations.FILTERED.txt",
    log:
        "scan2/{rescue_group}_mutations.log"
    benchmark:
        "scan2/{rescue_group}_mutations.benchmark.txt"
    params:
        column_name='donor',
        chrom_column='2',
        filter_column='16'
    localrule: True
    threads: 1
    resources:
        mem_mb=500,
        localjob=1
    shell:
        """
        {input.scan2_dir}/bin/digest_calls.R \
            --muts {input.muts} \
            --metadata {input.metadata} \
            --individual-column {params.column_name} \
            {output.unfiltered} &> {log}

        awk -F, 'NR == 1 || (${params.chrom_column} != "X" && ${params.chrom_column} != "Y" && ${params.filter_column} == "FALSE")' {output.unfiltered} > {output.filtered}
        """


rule scan2_table_to_csv:
    input:
        csv="scan2/{rescue_group}_mutations.{filter}.txt"
    output:
        csv="tables/{rescue_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{rescue_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        # Disable per-sample filtering by providing a superset of all samples.
        samples=config['all_samples']  
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


rule scan2_combine_all_tables:
    input:
        txts=expand('scan2/{rescue_group}_mutations.{{filter}}.txt',
            rescue_group=config['scan2_rescue_groups'].keys())
    output:
        csv="tables/all___{filter}_mut___any.csv"
    log:
        "tables/all___{filter}_mut___any.log"
    threads: 1
    localrule: True
    resources:
        mem_mb=250,
        localjob=1
    shell:
        """
        (head -1 {input[0]} ; \
         tail --quiet -n +2 {input} \
            | sort -t, -k2 -k3 -n) \
            > {output.csv}
        """


rule make_synthetic_group_table:
    input:
        csv="tables/all___{filter}_mut___any.csv"
    output:
        csv="tables/{synthetic_group}___{filter}_mut___{qualtype}.csv"
    log:
        "tables/{synthetic_group}___{filter}_mut___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=lambda wildcards: config['groups'][wildcards.synthetic_group]
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_csv.R"


# Some tools, e.g. SnpEff, may need VCF input
rule scan2_table_to_vcf:
    input:
        csv="tables/{rescue_or_synthetic_group}___{filter}_mut___{qualtype}.csv"
    output:
        vcf="vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.vcf"
    log:
        "vcfs/{rescue_or_synthetic_group}___{filter}___{qualtype}.log"
    params:
        qualtype='{qualtype}',
        filter='{filter}',
        samples=config['all_samples']  # Don't filter by sample. Also config['all_samples'] is a superset, so fairly meangingless
    threads: 1
    resources:
        mem_mb=1000
    script:
        "scripts/scan2_table_to_vcf.R"



# Older pipelines generated permutations on rescue groups, which happen to be mutually
# exclusive sets of all samples. The old code was basically a hack to generate permutations
# per sample, and even still often failed because the combine_permutations step of
# scan2::permtool has a wildly variable memory requirement (scales by mutation count, not
# sample count). To make matters worse, when combine_permutations failed, all of the per
# sample permutations were deleted because they are outputs of the same rule.
# 
# We now make use of SCAN2's --no-combine-permutations option to stop permtool after
# generating permutations per sample rather than using the previous hack.
#
# An added benefit is that all permutation groups are now created the same way (see rule
# scan2_permtool_synthetic) rather than having two different ways, one for rescue groups
# and one for all other "synthetic" groups.
#
# NOTE ON RANDOM SEEDS FOR PERMUTATIONS: scan2 generates random seeds per sample by the
# sample name alone. As long as all sample names are unique, there should be no issues.
#
# NOTE ON GENERATING PERMUTATIONS PER SAMPLE WITH SEPARATE RULES:
#   - do not try to run a "single sample" scan2 permtool. there is no good way to do this
#     since each permtool controller job ties up 1 CPU core by itself and does nothing.
#     one could try to instead have the controller job not submit to the cluster, but
#     then the cores given to that job have to be split across the 4 permutation types
#     (snv, indel) x (pass, rescue). This becomes intolerable for some extreme samples that
#     take ~16 hours to generate one permutation set. The wait time is too much whether
#     done serially (i.e., using all supplied cores but 1 job at a time) or in parallel
#     (i.e., supplying, e.g., 16 cores and targeting 4 cores per job).
#   - it is also inadvisable to add a "toolbox command" to scan2 that just creates one
#     permutation because single cells must be matched to bulks when creating the callable
#     regions BED file. this is possible, but we have to match the way SCAN2 chooses bulks
#     in the rare cases where #bulks > 1, and that logic does not belong in this pipeline.
print("WARNING: scan2_permtool: ensure that all single cell sample names are unique. Random seeds for permutation generation are created based on sample name!\n")
rule scan2_permtool_setup_all_samples:
    input:
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        genome_file="resources/hg19.genome",
        # reference genome is a hack to work around unintended behavior with chrY detection
        ref='resources/human_g1k_v37_decoy.fasta',
        # have to map back to original SCAN2 run for joint depth matrix and rescue group for recurrence-filtered muts
        muts="tables/all___FILTERED_mut___any.csv"
    output:
        "scan2/permtool_by_sample/scan.yaml"
    log:
        "scan2/permtool_by_sample/setup.log"
    benchmark:
        "scan2/permtool_by_sample/setup.benchmark.txt"
    params:
        outdir="scan2/permtool_by_sample",
        n_permutations=10000,
        # without sorted(), this rule is always rerun. I have no idea why the list comprehension
        # below would not be the same every time.
        sample_flags=' '.join(sorted(f"--permtool-sample {sample} scan2/{config['sample_to_donor_map'][sample]}/scan2" for sample in config['all_single_cells_in_groups']))
    localrule: True
    threads: 1
    resources:
        mem_mb=1000
    shell:
        """
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} init
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} config \
            --verbose \
            --scripts $(realpath {input.scan2_scripts}) \
            --ref {input.ref} \
            --analysis permtool \
            --no-combine-permutations \
            --permtool-muts {input.muts} \
            --permtool-bedtools-genome-file {input.genome_file} \
            --permtool-n-permutations {params.n_permutations} \
            {params.sample_flags}
        {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} validate
        """


rule scan2_permtool_run_all_samples:
    input:
        scan2_bin=ancient("scan2/SCAN2_specific_commit/bin/scan2"),
        scan2_snakefile=ancient("scan2/SCAN2_specific_commit/snakemake/Snakefile"),
        scan2_scripts="scan2/SCAN2_specific_commit/scripts",
        yaml="scan2/permtool_by_sample/scan.yaml"
    output:
        expand("scan2/permtool_by_sample/perms_by_sample_copy/{sample}/{muttype}_{passtype}.rda",
            sample=config['all_single_cells_in_groups'],
            muttype=[ 'snv', 'indel'], passtype=[ 'pass', 'rescue' ])
    # log: and benchmark: can't be used because they must contain the {sample} wildcards,
    # but only one log/benchmark file should be created for the whole run, not one per sample
    params:
        outdir="scan2/permtool_by_sample/"
    threads: 1
    resources:
        mem_mb=4000,
        make_permutations_n_cores=8,
        permtool_slurm_account='park_contrib',
        permtool_slurm_partition='priopark',
        permtool_slurm_runtime=4330
    shell:
        # --restart-times=2: critical parameter to allow memory step-up in make_permutations
        #
        # perms_by_sample_copy hack: major hack to prevent this rule from deleting all of the
        #   successful permutations it generates when >=1 fails. because the perms_by_sample
        #   directory is not listed in the output: of this rule, the files generated there
        #   will be safe. When the rule is successful, the copied output will all be created.
        """
            {input.scan2_bin} -d {params.outdir} --snakefile {input.scan2_snakefile} permtool \
                --joblimit 5000 \
                --make-permutations-n-cores {resources.make_permutations_n_cores} \
                --snakemake-args ' --restart-times 2 --keep-going --max-status-checks-per-second 0.1 --max-jobs-per-second 2 --slurm --default-resources slurm_account={resources.permtool_slurm_account} slurm_partition={resources.permtool_slurm_partition} runtime={resources.permtool_slurm_runtime}' >& {params.outdir}/run.log

            cp -ran {params.outdir}/perms_by_sample {params.outdir}/perms_by_sample_copy
        """


# OLD NOTES below. Now that all permutations are generated per sample, all groups
# of permutations are generated by this rule.
#
# left for posterity ------------------------------------------------
#
# To save compute resources, only run SCAN2 permtool on the exhaustive, mutually
# exclusive rescue groups. To create more permutation groups for, e.g., batch1 vs.
# batch2 analysis or bootstrapping/subsampling analyses, the per-sample output of
# previous permtool runs from the exhaustive, mutually exclusive runs are reused.
# These additional permutation groups are called "synthetic groups", and the
# definition in Snakefile:wildcard_constraints will define exactly which groups
# are recognized as synthetic.
#
# IMPORTANT: spending the CPU time to rerun permtools for additional permutation
# groups should provide the SAME OUTPUT as reusing the old permutations because
# the set of somatic mutation calls does not change and scan2 permtool also would
# use the same sequence of random seeds.
rule scan2_permtool_combine:
    input:
        combine_script="scan2/SCAN2_specific_commit/scripts/combine_permutations.R",
        rdas=lambda wildcards:
            [ "scan2/permtool_by_sample/perms_by_sample_copy/perms_by_sample/" + sample + "/{muttype}_{passtype}.rda"
                for sample in config['groups'][wildcards.group] ]
    output:
        perms="scan2/permtool/{group}/perms_{muttype}_{passtype}.rda",
        seeds="scan2/permtool/{group}/seedinfo_{muttype}_{passtype}.rda"
    log:
        "scan2/permtool/{group}/perms_{muttype}_{passtype}.log",
    benchmark:
        "scan2/permtool/{group}/perms_{muttype}_{passtype}.benchmark.txt",
    params:
        genome='hs37d5'
    # For memory step-up jobs, threads>1 can be counterproductive. R::future does not
    # alawys immediately detect when its fork()ed jobs are killed by the OOM killer,
    # thus allowing the surviving fork()ed jobs to run to completion before throwing
    # an error. I believe there is a way to enable a fast-failure mode in R::future,
    # but don't have time to look into that now.
    #
    # For now, 1 thread will do. The longest combine script only takes ~2 hours and
    # the vast majority are <15 minutes.
    threads: 1
    resources:
        # Memory usage of this combine script can be astronomical with too many cores:
        # permutation files (10,000 copies of each mutation set) are already large and
        # the r-future library we use for multithreading multiplies memory use by number
        # of threads.
        #
        # Threading aside, the combine script's memory usage scales directly off of number of mutations.
        # It'd take a little more work to get number of mutations here, so instead use
        # number of samples, mutation type and pass type as a proxy for number of
        # mutations. snvs are typically more frequent than indels, rescue typically
        # increases number by 40% (snvs) - 100% (indels)
        #
        # I built a very crude model of memory usage from a large run of 575 non-diseased
        # single cells:
        #
        # max memory usage (max_rss) / n_cores / n_samples
        #    muttype passtype       50%      80%      90%      99%     100%
        #     <char>   <char>     <num>    <num>    <num>    <num>    <num>
        # 1:   indel     pass 104.62656 179.3239 311.0429 597.4522 599.2750
        # 2:     snv     pass 269.90417 398.1473 485.2819 832.7726 840.6350
        mem_mb=lambda wildcards, input, threads, attempt: 1000 + \
            2**(attempt-1) * 
            threads * len(input.rdas) *
            1.5**(wildcards.passtype == 'rescue') * \
            2**(wildcards.muttype == 'snv') * 150
    shell:
        """
        {input.combine_script} \
            {params.genome} \
            {output.perms} \
            {output.seeds} \
            {threads} \
            {input.rdas} >& {log}
        """
