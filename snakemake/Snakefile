# vim: syntax=python

# >6.0 is required for modules
from snakemake.utils import min_version
min_version("6.0")

import re
import itertools
import yaml
import pandas as pd
import pprint as pp

config['nmf_min_sigs'] = 1
config['nmf_max_sigs'] = 8

# Identifiers recognized and/or created by SigProfilerExtractor. The notable differences
# are ID415 gets translated to CH415. There is no consistent way to handle this disagreement
# because SigProfilerMatrixGenerator uses "ID415", not "CH415".
# In addition to the ID->CH issue, I also have a "_corrected" version which accounts for
# SCAN2-specific indel sensitivity differences between channels that, of course, would
# not be supported by SPE.
# Map the nomenclature of SigProfilerExtractor to mine
def spe_type(s):
    return s.replace("ID415", "CH415").replace("_corrected", "")

config['my_sigprofiler_spectypes'] = [ 'SBS96', 'SBS384', 'ID83', 'ID83_corrected', 'ID415', 'ID415_corrected' ]
config['sigprofiler_spectypes'] = [ spe_type(s) for s in config['my_sigprofiler_spectypes'] ]

all_indel_channels = [
    "1:Del:C:0", "1:Del:C:1", "1:Del:C:2", "1:Del:C:3", "1:Del:C:4", "1:Del:C:5",
    "1:Del:T:0", "1:Del:T:1", "1:Del:T:2", "1:Del:T:3", "1:Del:T:4", "1:Del:T:5",
    "1:Ins:C:0", "1:Ins:C:1", "1:Ins:C:2", "1:Ins:C:3", "1:Ins:C:4", "1:Ins:C:5",
    "1:Ins:T:0", "1:Ins:T:1", "1:Ins:T:2", "1:Ins:T:3", "1:Ins:T:4", "1:Ins:T:5",
    "2:Del:R:0", "2:Del:R:1", "2:Del:R:2", "2:Del:R:3", "2:Del:R:4", "2:Del:R:5",
    "3:Del:R:0", "3:Del:R:1", "3:Del:R:2", "3:Del:R:3", "3:Del:R:4", "3:Del:R:5",
    "4:Del:R:0", "4:Del:R:1", "4:Del:R:2", "4:Del:R:3", "4:Del:R:4", "4:Del:R:5",
    "5:Del:R:0", "5:Del:R:1", "5:Del:R:2", "5:Del:R:3", "5:Del:R:4", "5:Del:R:5",
    "2:Ins:R:0", "2:Ins:R:1", "2:Ins:R:2", "2:Ins:R:3", "2:Ins:R:4", "2:Ins:R:5",
    "3:Ins:R:0", "3:Ins:R:1", "3:Ins:R:2", "3:Ins:R:3", "3:Ins:R:4", "3:Ins:R:5",
    "4:Ins:R:0", "4:Ins:R:1", "4:Ins:R:2", "4:Ins:R:3", "4:Ins:R:4", "4:Ins:R:5",
    "5:Ins:R:0", "5:Ins:R:1", "5:Ins:R:2", "5:Ins:R:3", "5:Ins:R:4", "5:Ins:R:5",
    "2:Del:M:1",
    "3:Del:M:1", "3:Del:M:2",
    "4:Del:M:1", "4:Del:M:2", "4:Del:M:3", "5:Del:M:1",
    "5:Del:M:2", "5:Del:M:3", "5:Del:M:4", "5:Del:M:5"
]
# Just 1bp (orange) deletions
just_1del = [ '1:Del:C:' + str(i) for i in range(0, 6) ] + [ '1:Del:T:' + str(i) for i in range(0, 6) ]
# Just 2bp deletions. There are two types: repeats (R) and microhomology (M)
just_2del = [ '2:Del:R:' + str(i) for i in range(0, 6) ] + [ '2:Del:M:1' ]
# Just 3bp microhomology deletions, a large part of the pink peak signature
just_3delMH = [ '3:Del:M:' + str(i) for i in [ 1, 2 ] ]

# OLD COMMENT FROM rule enrichment moved here  --------------------------
# Subset "signature-like" enrichment analysis - for signatures that are
# almost exclusively defined by mutations in a few channels (like SBS1
# being defined by C>Ts at the 4 CpG contexts), filter the mutation sets
# and permutation sets down to just those channels and run regular
# enrichment analyses on the filtered data.
# -----------------------------------------------------------------------
# If non-empty, signature-like enrichment analyses will be run on each element
# of "A" and "indel_A".
# It is not appropriate to use AB mutations here since they are biased for specific
# mutsig channels.
config['mutsig_subsets'] = {
    'indel_A' : {
        '1del' : just_1del,
        'not_1del' : set(all_indel_channels) - set(just_1del),
        '2del' : just_2del,
        'not_2del' : set(all_indel_channels) - set(just_2del),
        # There are not enough 3delMH indels to matter
        #'3delMH' : just_3delMH,
        #'not_3delMH' : set(all_indel_channels) - set(just_3delMH),
        '2del_3delMH' : just_2del + just_3delMH,
        'not_2del_3delMH' : set(all_indel_channels) - set(just_2del + just_3delMH),
        # Remove the first pink peak since it isn't as prevalent in ID4 as it is in ID83A
        '2del_noR0_3delMH' : set(just_2del + just_3delMH) - set([ '2:Del:R:0' ]),
        '2delR0': [ '2:Del:R:0' ],
        'not_2del_noR0_3delMH' : set(all_indel_channels) - set(just_2del + just_3delMH) #) | set([ '2:Del:R:0' ])
    }
}
#pp.pprint(config['mutsig_subsets'])
config['mutsig_subset_qualtypes'] = sum([
    [ qualtype + '_' + subset_name for subset_name in subsets ]
            for qualtype, subsets in config['mutsig_subsets'].items() ], [])
config['mutsig_subset_final_fragments'] = sum([
    [ subset_name for subset_name in subsets ]
            for qualtype, subsets in config['mutsig_subsets'].items() ], [])
#pp.pprint(config['mutsig_subset_qualtypes'])

# Creates a package name -> dict() map for each line.
# dict() contains installed_commit_id and actual_commit_id
# installed_commit_id is the parameter for snakefile.scan2 installation and reflects
# the version at install time.  If a newer commit that does not change any
# of the outputs of scan2 makepanel or call_mutations is used, update the actual_commit_id
# line instead.  actual_commit_id DOES NOT CONTROL ANYTHING, IT IS ONLY FOR RECORD
# KEEPING.
config['scan2_version'] = pd.read_csv('metadata/scan2_version_tracking.csv').set_index('package').T.to_dict('dict')

# Creates a donor ID -> dict() map for each line in scan2_resources.csv
# The dict for each donor contains named SCAN2 resource values
config['scan2_resources'] = pd.read_csv('metadata/scan2_resources.csv').set_index('donor').T.to_dict('dict')


# Important note on the `config' dictionary: consolidate all global variables so
# they can be robustly passed to module calls.
config['metadata'] = pd.read_csv('metadata/immutable_metadata.csv')
meta = config['metadata']  # Just for convenience within this Snakefile
config['sample_amp'] = dict(zip(list(meta['sample']), list(meta['amp'])))


# Never directly use donor_metadata - always go through config[donor_X]
# to enable data passing between modules.
donor_metadata = pd.read_csv('metadata/immutable_donor_metadata.csv')
config['donor_sex'] = dict(zip(list(donor_metadata['donor']), list(donor_metadata['sex'])))
config['donor_age'] = dict(zip(list(donor_metadata['donor']), list(donor_metadata['age'])))
config['donor_phenotype'] = dict(zip(list(donor_metadata['donor']), list(donor_metadata['phenotype'])))

# This only applies to building the alignability tiles
config['chrs_for_alignability'] = [ str(x) for x in range(1, 23) ] + [ 'X', 'Y' ]

# List of BAMs and bulk vs. single cell status. There is only one added
# bit of data in this file that is not available from config['metadata']:
# how to deal with multiple bulk BAMs in a single donor. In this case,
# the first listed bulk BAM in bams.yaml is treated as the matched bulk
# and the other BAM is simply included in GATK runs for downstream
# analysis. This file allows the user to modify this as desired in a
# slightly more straightforward way, though this ordering could also be
# assumed for immutable_metadata.csv as well.
with open('metadata/bams.yaml', 'r') as yf:
    bams = yaml.load(yf, Loader=yaml.FullLoader)
# `bams` is a dict mapping donor ID -> list(single cell bams, bulk bams, other bams), where
# each of the elements in the above list are also dicts mapping sample ID -> bam path.
config['bams'] = bams


config['all_samples'] = list(meta['sample'])
config['panel_samples'] = list(meta[meta['include.in.panel'] == True]['sample'])

# Just single cell samples
# XXX: DELETEME: Not used anywhere
#config['all_single_cells'] = list(meta[(meta['amp'] != "bulk")]['sample'])
config['scan2_single_cells'] = list(meta[(meta['analyze.with.scan2'] == True) & (meta['amp'] != "bulk")]['sample'])

# XXX: no longer used - these cells are removed in metadata/{sample_metadata.csv,groups.csv}
# This file allows the user to specify samples that were such outliers that they
# need to be completely removed from all analyses downstream of SCAN2 call_mutations.
with open('metadata/cell_exclusion_post_scan2.txt', 'r') as infile:
    for sample in infile:
        sample = sample.strip()
        print('REMOVING single cell sample "' + sample + '" from all analyses beyond scan2 call_mutations.')
        config['scan2_single_cells'].remove(sample)
        
# Same as above, but instead of a flat list of sample names, flat list of BAM paths
# final.bam in immutable_metadata.csv is not the link in bams.yaml
config['all_bams'] = sum([ list(bams[k][bt].values()) for k in bams.keys() for bt in bams[k].keys()  ], [])
config['panel_bams'] = [ bams[k][bt][s] for k in bams.keys() for bt in bams[k].keys() for s in bams[k][bt].keys() if s in config['panel_samples'] ]

config['all_donors'] = list(dict.fromkeys(list(meta['donor'])))
config['scan2_donors'] = list(dict.fromkeys(list(meta[meta['analyze.with.scan2'] == True]['donor'])))

config['sample_to_donor_map'] = dict(zip(list(meta['sample']), list(meta['donor'])))
config['donor_to_scan2_sample_map'] = dict(
    (donor, list(meta[(meta['analyze.with.scan2'] == True) & (meta['donor'] == donor)]['sample']))
        for donor in set(meta[meta['analyze.with.scan2'] == True]['donor']))

# Various mappings of donor/sample to SCAN2 outputs
# These depth matrices should be outputs of snakemake.scan2:scan2_call_mutations_run,
# not specified here in a config file.
config['scan2_depth_matrices'] = dict((donor, "scan2/" + donor + "/scan2/depth_profile/joint_depth_matrix.tab.gz") for donor in config['scan2_donors'])

# map: sample -> summary SCAN2 object
config['scan2_summary_object'] = \
    dict(zip(config['scan2_single_cells'], [ 'scan2/summary_objects/' + s + '.rda' for s in config['scan2_single_cells'] ]))

# map: sample -> full SCAN2 object
config['scan2_full_object'] = \
    dict(zip(config['scan2_single_cells'], [ 'scan2/full_objects/' + s + '.rda' for s in config['scan2_single_cells'] ]))




############################################################################################
# Define groups of single cells for various analyses.
#
# NOTE: currently, only samples analyzed by SCAN2 are permitted for groups. Support for
# external mutation calls may be added in the future.
#
# Column orders must be respected.
#   metadata/groups.csv
#       - 2 column file: group, sample name
#   metadata/group_metadata.csv
#       - 3 column file: group, analysis name, color
#       contains metadata concerning which groups participate in which analyses.
#       analysis name="scan2_rescue" is required (explained below).
#       Presence of a row indicates the group is included in the analysis,
#       absence indicates exclusion.  Groups may belong to multiple analyses.
# 
# Rescue groups
# -------------
# There is one special group type, the "rescue_group". This group is defined by setting
# analysis=scan2_rewscue in group_metadata.csv.  It must cover all single
# cells in this analysis, and the groups must be mutually exclusive.  SCAN2 
# mutation signature-based rescue will be run on these groups and post-SCAN2 recurrence
# filtering will occur on each group separately.
# 
# Rescue groups must be defined by single cells that presumably share the same mutational
# mechanisms (this can be checked by hand later using SCAN2's signature homogeneity tests).
# Rescue groups *are not* meant to define groups that are to be compared in downstream
# analysis (e.g., aging trends, mutation signature analysis, enrichment analyses and so on).
# Use generic_groups or synthetic_groups to create groups of cells for comparison.
#
# Note: although SCAN2 signature rescue is NOT DESIGNED for MDA amplified single cells and
# (at time of this writing) will not work well, it is useful to run MDA cells through
# rescue anyway to generate files in a uniform format.  After rescue is run on MDA cells,
# the rescued mutations are ignored.  This is not exactly equivalent to not running rescue
# because the recurrent mutation artifact filters (downstream of SCAN2) consider rescued
# calls when defining recurrence or clusters, but the effect should be minimal.
#
# DO NOT combine MDA cells with PTA cells.
#
# 
# Other groups
# ------------
# The user is allowed to create any other groups to facilitate comparisons.  These groups
# need not cover all cells nor be mutually exclusive.
############################################################################################

config['groups'] = {}
for group, df in pd.read_csv('metadata/groups.csv', comment='#').groupby('group'):
    # group is group name, df is a pandas dataframe with 'group' and 'sample' columns
    config['groups'][group] = list(df['sample'])

# Build the list of cells that appear in at least 1 group. Cells appearing in no
# groups are essentially unanalyzed downstream of SCAN2 calling. For example, there
# is no need to generate permutation objects for such cells.
config['all_single_cells_in_groups'] = []
# Ensure samples that were not analyzed by SCAN2 are not included in any group
for group, samples in config['groups'].items():
    # Add to the list of 
    config['all_single_cells_in_groups'] = config['all_single_cells_in_groups'] + samples
    invalid_cells = [ s for s in samples if s not in config['scan2_single_cells'] ]
    if len(invalid_cells) > 0:
        raise RuntimeError('only SCAN2-analyzed cells are permitted in groups.  the following cells in group ' + group + ' were not analyzed by SCAN2: ' + ' '.join(invalid_cells))

# uniquify
config['all_single_cells_in_groups'] = set(config['all_single_cells_in_groups'])

# Maps analysis name -> list(map group name -> metadata)
config['analysis_meta'] = {}
for analysis, df in pd.read_csv('metadata/group_metadata.csv').groupby('analysis'):
    config['analysis_meta'][analysis] = df.drop(columns='analysis').set_index('group').T.to_dict()

# dict maping: rescue group name -> dict(pre-rescue SCAN2 object -> sample name)
# it would be preferable if each rescue group could be mapped only to a list of sample
# names, but for technical reasons it is necessary to map object paths to sample names.
config['scan2_rescue_groups'] = {}
for group, _ in config['analysis_meta']['scan2_rescue'].items():
    samples = config['groups'][group]
    config['scan2_rescue_groups'][group] = dict(
          # construct path to each SCAN2 analysis object from call_mutations
        [ ('scan2/' + config['sample_to_donor_map'][s] + '/scan2/call_mutations/' + s + '/scan2_object.rda', s)
            for s in samples ]
    )

# reverse lookup: map sample -> rescue group
config['scan2_rescue_groups_reverse'] = \
    dict((sample, g) for g in config['scan2_rescue_groups'].keys() for sample in config['scan2_rescue_groups'][g].values())


############################################################################################
# Group comparisons
#
# Some analyses compare groups.  These comparisons are defined in
#     metadata/group_comparisons.csv
# Format: columns named: comparison, group, analysis
#       - any additional columns are treated as metadata, but are currently ignored
# Analyses that use group comparisons:
#   - aging_rates
############################################################################################

# Maps analysis name -> list(map group name -> metadata)
config['analysis_comparisons'] = {}
for analysis, df in pd.read_csv('metadata/group_comparisons.csv').groupby('analysis'):
    config['analysis_comparisons'][analysis] = \
        df.drop(columns='analysis').groupby('comparison')['group'].apply(list).to_dict()

config['generic_groups'] = []
config['synthetic_groups'] = list(set(config['groups'].keys()) - set(config['scan2_rescue_groups'].keys()))

config['de_novo_mutsig_groups'] = list(config['analysis_meta']['de_novo_mutsig'].keys())

config['enrichment_groups'] = list(config['analysis_meta']['enrichment'].keys())

config['cosmic_vs_age_group'] = dict()
config['comparison_groups'] = []
config['group_colors'] = dict()
config['sensitivity_groups'] = []
config['all_groups'] = []

snv_qualtypes = [ 'A' ] #, 'AB' ]
indel_qualtypes = [ 'indel_A' ] #, 'indel_AB' ]
base_qualtypes = snv_qualtypes + indel_qualtypes
all_qualtypes = base_qualtypes + config['mutsig_subset_qualtypes']




# Enrichment analyses are keyed by {group_name}___{qualtype}, i.e., pta_neuron___indel_A.
# These keys need to be mapped to mutation tables (i.e., actual SCAN2 calls), permutation
# objects containing mutation tables shuffled across the genome and (for analysis of
# signature enrichment) the COSMIC signatures determined to be active.
mutclass_to_mutfile = {}
mutclass_to_permfile = {}
mutclass_to_cosmic = {}
mutclass_to_denovo = {}
for group_name in list(config['scan2_rescue_groups'].keys()) + config['synthetic_groups']:
    for qualtype in all_qualtypes:
        analysis_key = group_name + "___" + qualtype

        # The active COSMIC signature set is determined from VAF-based (quality type A or indel_A)
        # SCAN2 calls, not signature-rescued calls that can have biased mutation signatures.
        cosmic_qualtype = qualtype
        if qualtype == 'AB':
            cosmic_qualtype = "A"
        if qualtype == 'indel_AB':
            cosmic_qualtype = "indel_A"
        muttype = "indel" if qualtype.startswith("indel") else "snv"
        passtype = "rescue" if qualtype == "AB" or qualtype == 'indel_AB' else "pass"
        # final_fragment supports mutsig subsetting (e.g., indel_A_nameofsubset)
        # if qualtype is a base_qualtype, then final_fragment=''
        final_fragment = re.sub(pattern='|'.join([ '^' + q for q in base_qualtypes]), repl='', string=qualtype, count=1)
        mutclass_to_mutfile[analysis_key] = "tables/" + group_name + "___FILTERED_mut___" + qualtype + ".csv"
        mutclass_to_permfile[analysis_key] = "scan2/permtool/" + group_name + "/perms_" + muttype + "_" + passtype + final_fragment + ".rda"
        if analysis_key not in mutclass_to_cosmic.keys():
            mutclass_to_cosmic[analysis_key] = {}
        mutclass_to_cosmic[analysis_key]['sigprofilerextractor'] = "mutsigs/sigprofilerextractor/cosmic_reduced___" + cosmic_qualtype + ".csv"

        if analysis_key not in mutclass_to_denovo.keys():
            mutclass_to_denovo[analysis_key] = {}
        mutclass_to_denovo[analysis_key]['sigprofilerextractor'] = "mutsigs/sigprofilerextractor/denovo___" + cosmic_qualtype + ".csv"


# The output directory and signal files are the only parameters that should
# change between analyses.
#
# Enrichment will be computed on all genome tile sets (corresponding to bin
# sizes) and qsizes specified here, so only list what needs to be computed.
# Other genome tile sets (like the 1MB tiles used for cancer) should not be
# listed here.
#
# If a different tile set is needed for a specific module (e.g., for cancer
# SNV density), make a different dictionary.  This is the default setup.
config['enrichment_config'] = {
    'qbed_from_bigwig_script': 'snakemake/scripts/make_qbed_from_bigwig.sh',
    'quantiles': [ '3', '5', '10' ],
    'tiles': {
        '1000':    'alignability/genome_tiles/genome_tiles_1000binsize.bed'
    },
    'masks': {
        '1000':    'alignability/genome_tiles/genome_mask_1000binsize.bed'
    },
    'mutclass_to_mutfile': mutclass_to_mutfile,
    'mutclass_to_permfile': mutclass_to_permfile,
    'mutclass_to_cosmic': mutclass_to_cosmic,
    'mutclass_to_denovo': mutclass_to_denovo
}



#################################################################################
# Data sources that provide genomic covariates for enrichment analyses.
#################################################################################

# Min coverage ("mc") values 02 = 20%, 08 = 80% mean that only bins
# covered >=20% or >=80% (these are separate analyses) by the relevant data
# sources are included when computing
# quantiles.  I.e., for gene expression from GTEx, genomic bins that are not
# transcribed at all are completely excluded before calculating quantiles of
# expression.  If these bins were included, the expression distribution would
# have a very large spike at expression=0.
# 
# Min coverage values: when using very small bins (like 1kb), the exact min
# coverage value has little effect (data not shown).  It's essentially binary
# for =0 or >0. The value matters more for large bins like the 1 MB bins used
# for cancer analysis. In that case, requiring 80% coverage excludes ~90% of
# the genome.
#
# IMPORTANT: the average bin value we compute accounts for coverage by assigning
# a value of 0 to any bases not covered. E.g., a bin with 450 bp covered by gene
# G and 550 bp not transcribed would be averaged such that
#    bin-wide average expression value = 0.45*(G's expression # level) + 0.55*0
#
# Sources no longer used:
#    'encode_replichip'
#quantile_datasources = [ 'gtex_expression_mc' + x for x in [ '02', '08' ] ] + \
                #[ 'scrnaseq_expression_mc' + x for x in [ '02', '08' ] ] + \
quantile_datasources = [
    'gene_length',
    'gtex_expression_mc02', 'scrnaseq_expression_mc02',
    'nott', 'repliseq', 'conservation', 
    'scatacseq', 'cancer_snvdens', 'roadmap_histone_signal_brain' ]

config['quantile_beds_for_sensitivity'] = []
with open('manifests/QBEDS_FOR_SENSITIVITY_ANALYSIS.MANIFEST', 'r') as f:
    for line in f:
        config['quantile_beds_for_sensitivity'].append(line.strip())
        

# Some other data sources no longer used:
#     'boca2', 'hic_tads'
bed_region_datasources = [ 'dna_repair_hotspots',
                           'fragile_sites',
                            # 'gencode',   # can no longer allow this because it's a substring of gencode_simplified
                           # requires some hackery because the datasource internally recorded in BED
                           # files, enrichment objects, tables, etc. is "gencode", not "gencode_full".
                           # would need to rerun (with an updated GENCODE.MANIFEST) to fix this, but
                           # the time required to do that isn't worth the wait.
                           'gencode_full',
                           'gencode_simplified',
                           'gene_panels',
                           'gtex', 'gtex_genes', 'nott', 'roadmap_chromhmm_brain' ]

config['region_beds_for_sensitivity'] = []
with open('manifests/BEDS_FOR_SENSITIVITY_ANALYSIS.MANIFEST', 'r') as f:
    for line in f:
        config['region_beds_for_sensitivity'].append(line.strip())

print('------------- rescue or synthetic groups (' +
    str(len(list(config['scan2_rescue_groups'].keys()) + config['synthetic_groups'])) +
    ') -------------')
for g in sorted(list(config['scan2_rescue_groups'].keys()) + config['synthetic_groups']):
    print(g)
#print('allowed rescue_groups:' + '|'.join(config['scan2_rescue_groups'].keys()))
#print('allowed synthetic_groups:' + '|'.join(config['synthetic_groups']))
#print('chroms: ' + '|'.join([ str(i) for i in range(1,23) ] + [ 'X', 'Y' ]))
print(set(config['sigprofiler_spectypes']))
print('|'.join(set(config['sigprofiler_spectypes'])))
wildcard_constraints:
    # snakemake seems not to match empty strings for a wildcard
    underscore='|_',           # allow either empty string or an underscore
    dot='|\.',                  # allow either empty string or a dot
    chrom='|'.join(config['chrs_for_alignability']),
    corrected='|corrected',
    datasource='|'.join(quantile_datasources + bed_region_datasources),
    sigtype='quantile|bed_regions',
    analysistype='bedenrich|qbedenrich|sigenrich_sigprofilerextractor|sigenrich_ad_hoc_signature_selection',
    sigenrich_type='cosmic|denovo',
    mutsig_subset='|'.join(config['mutsig_subset_final_fragments']),
    mdatype='mda_gfap|mda_sox10',
    nmf_spectype='ID|SBS',
    # Below are identifiers recognized/created by SigProilerExtractor
    sigprofiler_spectype='|'.join(set(config['sigprofiler_spectypes'])),
    # "my_" versions use ID415 instead of CH415 and there are additional "_corrected" versions
    my_sigprofiler_spectype='|'.join(config['my_sigprofiler_spectypes']),
    filter='FILTERED|UNFILTERED',
    binsize='|'.join([ '100', '200', '1000', '1000000' ]),
    base_qualtype='|'.join(base_qualtypes),
    qualtype='|'.join(all_qualtypes),
    passtype='pass|rescue',  # A more updated term for 'A' and 'AB' types
    cosmic='cosmic_full|cosmic_reduced',
    cancer_project='tcga|icgc',
    mutsig_selection_method='ad_hoc_signature_selection|sigprofilerextractor',
    muttype='snv|indel',
    donor='|'.join(config['all_donors']),
    tumor='|'.join(config['pcawg_tumors']),
    # Almost all of these group types are no longer relevant - need to cull out the unused stuff.
    #group='|'.join(config['groups'].keys()),
    group='|'.join(list(config['scan2_rescue_groups'].keys()) + config['synthetic_groups']),
    generic_group='|'.join(config['generic_groups']),
    rescue_group='|'.join(config['scan2_rescue_groups'].keys()),
    synthetic_group='|'.join(config['synthetic_groups']),
    # same as above, but allows for {synthetic_group1}_vs_{synthetic_group2} style outputs
    synthetic_group1='|'.join(config['synthetic_groups']),  
    synthetic_group2='|'.join(config['synthetic_groups']),
    any_group='|'.join(config['all_groups']),
    # same as above
    any_group1='|'.join(config['all_groups']),
    any_group2='|'.join(config['all_groups']),
    rescue_or_synthetic_group='|'.join(list(config['scan2_rescue_groups'].keys()) + config['synthetic_groups']),
    #mutsig_group='|'.join(config['generic_groups']),
    cosmic_vs_age_group='|'.join(config['cosmic_vs_age_group'].keys()),
    # These types are from GENCODE's classification. This subset includes all types whose genomic
    # footprint is > 10 Mb, which is a reasonable requirement for enrichment analysis.
    transcript_type='|'.join([ 'antisense', 'lincRNA', 'processed_transcript', 'protein_coding', 'transcribed_unprocessed_pseudogene' ])


rule scan2_panel_setup:
    input:
        "scan2/panel/makepanel/scan.yaml"

rule scan2_panel_run:
    input:
        "scan2/panel/makepanel/panel/panel.tab.gz",
        "scan2/panel/makepanel/makepanel_collected_benchmarks.txt"

rule scan2_setup:
    input:
        expand("scan2/{donor}/scan2/scan.yaml", donor=config['scan2_donors'])

def scan2_unpack_donors_and_samples(bams):
    ret = []
    for donor in config['scan2_donors']:
        ret = ret + expand("scan2/{donor}/scan2/call_mutations/{sample}/scan2_object.rda",
            donor=donor,
            sample=list(meta[(meta['donor'] == donor) & (meta['amp'] != 'bulk')]['sample']))
        ret = ret + [ "scan2/" + donor + "/scan2/depth_profile/joint_depth_matrix.tab.gz" ]
        ret = ret + [ "scan2/" + donor + "/scan2/depth_profile/joint_depth_matrix.tab.gz.tbi" ]
    return(ret)

rule scan2_call:
    input:
        scan2_unpack_donors_and_samples(bams)

rule scan2_rescue:
    input:
        expand('scan2/rescue_{rescue_group}/rescued_muts.txt',
            # XXX: CHECKME: not sure if using .keys() here is correct. previously used
            # global variable scan2_rescue_groups rather than keying into config[]
            rescue_group=config['scan2_rescue_groups'].keys()),
        expand('scan2/rescue_{rescue_group}/sig_homogeneity_tests.txt',
            rescue_group=config['scan2_rescue_groups'].keys())

rule scan2_summarize:
    input:
        expand('scan2/summary_objects/{sample}.rda',
            sample=config['scan2_single_cells'])

rule scan2_digest:
    input:
        expand('scan2/{rescue_group}_mutations.{filter_status}.txt',
            rescue_group=config['scan2_rescue_groups'].keys(),
            filter_status=[ 'FILTERED', 'UNFILTERED' ])

# For development. scan2_permtool creates these permutations and then
# combines them into single permutation objects.
rule scan2_permtool_by_sample:
    input:
        expand('scan2/permtool_by_sample/perms_by_sample_copy/{sample}/{muttype}_{passtype}.rda',
            sample=config['all_single_cells_in_groups'],
            muttype=[ 'snv', 'indel' ],
            passtype=[ 'pass', 'rescue' ])

rule scan2_permtool:
    input:
        expand('scan2/permtool/{group}/perms_{muttype}_{passtype}.rda',
            # Only enrichment analyses require permutations
            group=config['enrichment_groups'],
            muttype=[ 'snv', 'indel' ],
            passtype=[ 'pass', 'rescue' ])

rule scan2_objects:
    input:
        expand('scan2/{size}_objects/{sample}.rda',
            size=[ 'full', 'summary' ], sample=config['scan2_single_cells'])


rule alignability:
    input:
        expand('alignability/genome_tiles/genome_mask_{binsize}binsize.bed',
            binsize=[ 200, 1000, 1000000 ]),
        expand("alignability/bigwigs_binsize{binsize}/{sample}.bw",
            # 100 for alignability classifier, 200 for ChromHMM tracks,
            # 1000 and 1M for enrichment analyses of all quantitative covs.
            binsize=[ 100, 200, 1000, 1000000 ],
            sample=config['scan2_single_cells'])


rule unfiltered_muts:
    input:
        "tables/all___UNFILTERED_mut___any.csv"


rule aging_rates:
    input:
        "aging_rates/mutation_burdens_long_table.csv"


# Run de novo signature extraction with SigProfilerExtractor
spe_root = 'mutsigs/sigprofilerextractor'
spe_one_N_template_root = spe_root + '/{{de_novo_group}}/{my_type}/{{N}}_signatures/{spe_type}/'

rule mutsigs:
    input:
        expand(spe_root + '/{de_novo_group}/{my_sigprofiler_spectype}/All_solutions_stat.csv',
            de_novo_group=config['de_novo_mutsig_groups'],
            my_sigprofiler_spectype=config['my_sigprofiler_spectypes']),
        expand(spe_root + '/{de_novo_group}/{my_sigprofiler_spectype}/Most_Stab_Sigs/{file}',
            file=[ 'exposures.csv', 'exposures_SEM.csv', 'signatures.csv', 'signatures_SEM.csv',
                   'cosmic_exposures.csv', 'cosmic_signatures.csv', 'cosmic_muttype_probabilities.csv', 'cosmic_stats.csv',
                   'stats_convergence.txt', 'stats_samples.txt', 'stats_signatures.txt' ],
            de_novo_group=config['de_novo_mutsig_groups'],
            my_sigprofiler_spectype=config['my_sigprofiler_spectypes'])


# PRIMARILY FOR TESTING AND DEVELOPMENT - most users should not need to run this rule
# directly. Use rule mutsigs above.
# Runs SigProfilerExtractor in parallel with each job handling one value of N, the
# total number of signatures.
# This requires a special gather job at the end to combine individual outputs
# for each N and select an optimal signature.
rule mutsigs_scatter:
    input:
        expand(
            expand([ spe_one_N_template_root + 'All_solutions_stat.csv',
                     spe_one_N_template_root + 'All_Solutions/{spe_type}_{{N}}_Signatures/Activities/{spe_type}_S{{N}}_NMF_Activities.txt',
                     spe_one_N_template_root + 'All_Solutions/{spe_type}_{{N}}_Signatures/Signatures/{spe_type}_S{{N}}_Signatures.txt' ],
                zip,
                my_type=config['my_sigprofiler_spectypes'],
                spe_type=config['sigprofiler_spectypes']),
            de_novo_group=config['de_novo_mutsig_groups'],
            N=list(range(config['nmf_min_sigs'], config['nmf_max_sigs']+1))
        )


# Enrichment analyses of total mutation burden and ID-A-like deletions
rule enrichment:
    input:
        # All quantile-based regions
        expand('enrichment/{datasource}/quantile/{group}___{qualtype}.{nquantiles}quantiles.{output}',
            group=config['enrichment_groups'],
            qualtype=all_qualtypes,
            nquantiles=[ 10 ],
            datasource=quantile_datasources,
            output=[ 'svg', 'pdf', 'csv' ]),
        # All regions directly specified by BED files
        expand('enrichment/{datasource}/bed_regions/{group}___{qualtype}.{output}',
            group=config['enrichment_groups'],
            qualtype=all_qualtypes,
            datasource=bed_region_datasources,
            output=[ 'svg', 'pdf', 'csv' ])



include: "snakefile.data"
include: "snakefile.scan2"
include: "snakefile.aging_rates"
include: "snakefile.mutsigs"
include: "snakefile.alignability"
include: "snakefile.scatacseq"
include: "snakefile.gtex_enrichment"
include: "snakefile.gene_length"
include: "snakefile.scrnaseq_enrichment"
include: "snakefile.nott_enrichment"
include: "snakefile.fragile_sites_enrichment"
include: "snakefile.dnarepair_enrichment"
include: "snakefile.gene_region_enrichment"
include: "snakefile.other_enrichment"


# This must be defined below all other rules to use the rules.XXX.output directive.
rule all:
    input:
        rules.misc.input,
        rules.scan2_metrics.input,
    default_target: True
